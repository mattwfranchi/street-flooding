{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import multiprocessing\n",
    "# only set fork if not already set\n",
    "if multiprocessing.get_start_method(allow_none=True) is None:\n",
    "    multiprocessing.set_start_method(\"fork\")\n",
    "import pandas as pd\n",
    "import stan\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matt Updates (8/25)\n",
    "- Got conda environment that runs this notebook installed successfully \n",
    "- Now, to plug in real data \n",
    "- Anyway to suppress some of this output? \n",
    "- tried to add some upweighting to account for the fact that the 1000-image sample is not representative of overall image counts, not sure if this was the right intuition? The r_hat goes down to 1.0 for all of the fields, but there's this 'metropolis' error that pops up like 20 times. \n",
    "- i think something definitely needs to be done? these values aren't correct (which is what happens when i put 670/330 in, the raw counts of annotated neg/pos)\n",
    "\n",
    "Working results that don't make sense \n",
    "- empirical_p_y 0.22638264951208253\n",
    "- empirical_p_yhat 0.059639051600278256\n",
    "- p_y_hat_1_given_y_1 0.21163348990040542\n",
    "- p_y_hat_1_given_y_0 0.015385530450161835\n",
    "- p_y_1_given_y_hat_1 0.8033352121400738\n",
    "- p_y_1_given_y_hat_0 0.1897914834156601\n",
    "- number of annotated classified negative which were positive: 132/670\n",
    "- number of annotated classified positive which were positive: 276/330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTLabel            0\n",
      "BoroCode           0\n",
      "BoroName           0\n",
      "CT2020             0\n",
      "BoroCT2020         0\n",
      "CDEligibil         0\n",
      "NTAName            0\n",
      "NTA2020            0\n",
      "CDTA2020           0\n",
      "CDTANAME           0\n",
      "GEOID              0\n",
      "PUMA               0\n",
      "Shape_Leng         0\n",
      "Shape_Area         0\n",
      "geometry           0\n",
      "total_images       0\n",
      "positive_images    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['CTLabel', 'BoroCode', 'BoroName', 'CT2020', 'BoroCT2020', 'CDEligibil',\n",
       "       'NTAName', 'NTA2020', 'CDTA2020', 'CDTANAME', 'GEOID', 'PUMA',\n",
       "       'Shape_Leng', 'Shape_Area', 'geometry', 'total_images',\n",
       "       'positive_images'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load actual reuslts \n",
    "ct_dataset = pd.read_csv(\"data/processed/flooding_ct_dataset.csv\")\n",
    "ct_dataset[['total_images','positive_images']] = ct_dataset[['total_images','positive_images']].astype(int).fillna(0)\n",
    "print(ct_dataset.isna().sum())\n",
    "ct_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emma notes (8/22):\n",
    "\n",
    "In general, let's build complexity iteratively. Start by getting reasonable results without worrying about ICAR prior/smoothing. Then use standard ICAR prior (with weight 0.5). Then use full CAR (maybe). Data generation code is reviewed + model without proper CAR prior is reviewed. Another thing it might be nice to implement at some point is using the information about where the annotated images are (i.e., what Census tracts). Could incorporate this as a multinomial (potentially?) \n",
    "\n",
    "Model with simple L2 smoothing (or no smoothing at all - stan_code_with_weighted_ICAR_prior):\n",
    "\n",
    "1. Consistently recovers parameters for realistic parameter settings (with no smoothing)\n",
    "2. Reviewed Stan code and looks good. \n",
    "3. ALso implemented L2 regularization for adjacent Census tracts. This isn't actually the \"proper\" way to do it, but might be useful on real data. Haven't tested how this performs. \n",
    "\n",
    "Model wih full CAR prior (stan_code_proper_car_prior):\n",
    "\n",
    "1. Haven't reviewed this or verified it recovers correct params (recently; I think I did a while ago). When you do review, don't need to re-review all the Bayesian conditioning math; it should be pretty similar to the old code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stan code with CAR prior seems to work but hasn't been reviewed. \n",
    "# CAR prior is https://mc-stan.org/users/documentation/case-studies/mbjoseph-CARStan.html\n",
    "stan_code_proper_car_prior = '''\n",
    "functions {\n",
    "  /**\n",
    "  * Return the log probability of a proper conditional autoregressive (CAR) prior \n",
    "  * with a sparse representation for the adjacency matrix\n",
    "  *\n",
    "  * @param phi Vector containing the parameters with a CAR prior\n",
    "  * @param tau Precision parameter for the CAR prior (real)\n",
    "  * @param alpha Dependence (usually spatial) parameter for the CAR prior (real)\n",
    "  * @param W_sparse Sparse representation of adjacency matrix (int array)\n",
    "  * @param n Length of phi (int)\n",
    "  * @param W_n Number of adjacent pairs (int)\n",
    "  * @param D_sparse Number of neighbors for each location (vector)\n",
    "  * @param lambda Eigenvalues of D^{-1/2}*W*D^{-1/2} (vector)\n",
    "  *\n",
    "  * @return Log probability density of CAR prior up to additive constant\n",
    "  */\n",
    "  real sparse_car_lpdf(vector phi, real tau, real alpha,\n",
    "                       array[,] int W_sparse, vector D_sparse, vector lambda,\n",
    "                       int n, int W_n) {\n",
    "    row_vector[n] phit_D; // phi' * D\n",
    "    row_vector[n] phit_W; // phi' * W\n",
    "    vector[n] ldet_terms;\n",
    "    \n",
    "    phit_D = (phi .* D_sparse)';\n",
    "    phit_W = rep_row_vector(0, n);\n",
    "    for (i in 1 : W_n) {\n",
    "      phit_W[W_sparse[i, 1]] = phit_W[W_sparse[i, 1]] + phi[W_sparse[i, 2]];\n",
    "      phit_W[W_sparse[i, 2]] = phit_W[W_sparse[i, 2]] + phi[W_sparse[i, 1]];\n",
    "    }\n",
    "    \n",
    "    for (i in 1 : n) {\n",
    "      ldet_terms[i] = log1m(alpha * lambda[i]);\n",
    "    }\n",
    "    return 0.5\n",
    "           * (n * log(tau) + sum(ldet_terms)\n",
    "              - tau * (phit_D * phi - alpha * (phit_W * phi)));\n",
    "  }\n",
    "}\n",
    "\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0> N_edges;\n",
    "  matrix<lower=0, upper=1>[N, N] W; // adjacency matrix\n",
    "  int W_n; // number of adjacent region pairs\n",
    "  array[N] int<lower=0> n_images; \n",
    "  array[N] int<lower=0> n_classified_positive; \n",
    "\n",
    "  //annotation sample. \n",
    "  int n_annotated_classified_negative;\n",
    "  int n_annotated_classified_positive;\n",
    "  int n_annotated_classified_negative_true_positive;\n",
    "  int n_annotated_classified_positive_true_positive;  \n",
    "}\n",
    "transformed data {\n",
    "  array[W_n, 2] int W_sparse; // adjacency pairs\n",
    "  vector[N] D_sparse; // diagonal of D (number of neigbors for each site)\n",
    "  vector[N] lambda; // eigenvalues of invsqrtD * W * invsqrtD\n",
    "  \n",
    "  {\n",
    "    // generate sparse representation for W\n",
    "    int counter;\n",
    "    counter = 1;\n",
    "    // loop over upper triangular part of W to identify neighbor pairs\n",
    "    for (i in 1 : (N - 1)) {\n",
    "      for (j in (i + 1) : N) {\n",
    "        if (W[i, j] == 1) {\n",
    "          W_sparse[counter, 1] = i;\n",
    "          W_sparse[counter, 2] = j;\n",
    "          counter = counter + 1;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  for (i in 1 : N) {\n",
    "    D_sparse[i] = sum(W[i]);\n",
    "  }\n",
    "  {\n",
    "    vector[N] invsqrtD;\n",
    "    for (i in 1 : N) {\n",
    "      invsqrtD[i] = 1 / sqrt(D_sparse[i]);\n",
    "    }\n",
    "    lambda = eigenvalues_sym(quad_form(W, diag_matrix(invsqrtD)));\n",
    "  }\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  vector[N] phi;\n",
    "  real<lower=0> tau;\n",
    "  real<lower=0, upper=1> alpha;\n",
    "  real <upper=0>phi_offset; // you may not want to place this constraint but it helps convergence with small numbers of samples\n",
    "  real<lower=0, upper=1> p_y_1_given_y_hat_1; \n",
    "  real<lower=0, upper=1> p_y_1_given_y_hat_0;\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[N] p_y = inv_logit(phi + phi_offset);\n",
    "    //real predicted_overall_p_y = sum(n_images .* p_y) / sum(n_images);\n",
    "    real empirical_p_yhat = sum(n_classified_positive) * 1.0 / sum(n_images);\n",
    "    real p_y_hat_1_given_y_1 = empirical_p_yhat * p_y_1_given_y_hat_1 / (empirical_p_yhat * p_y_1_given_y_hat_1 + (1 - empirical_p_yhat) * p_y_1_given_y_hat_0);\n",
    "    real p_y_hat_1_given_y_0 = empirical_p_yhat * (1 - p_y_1_given_y_hat_1) / (empirical_p_yhat * (1 - p_y_1_given_y_hat_1) + (1 - empirical_p_yhat) * (1 - p_y_1_given_y_hat_0));\n",
    "}\n",
    "model {\n",
    "  n_annotated_classified_negative_true_positive ~ binomial(n_annotated_classified_negative , p_y_1_given_y_hat_0);\n",
    "  n_annotated_classified_positive_true_positive ~ binomial(n_annotated_classified_positive, p_y_1_given_y_hat_1);\n",
    "  tau ~ gamma(2, 2);\n",
    "  phi ~ sparse_car(tau, alpha, W_sparse, D_sparse, lambda, N, W_n);\n",
    "  phi_offset ~ normal(-4, 0.5);\n",
    "  n_classified_positive ~ binomial(n_images, p_y .* p_y_hat_1_given_y_1 + (1 - p_y) .* p_y_hat_1_given_y_0);\n",
    "}\n",
    "'''\n",
    "\n",
    "# Stan code below works and has been reviewed but does not implement a proper ICAR prior. Set variable use_ICAR_prior to 0 if you don't want to use one at all. \n",
    "# I think it might also be principled to set the ICAR prior weight to 0.5, https://mc-stan.org/users/documentation/case-studies/icar_stan.html. \n",
    "stan_code_with_weighted_ICAR_prior = '''\n",
    "data {\n",
    "  int<lower=0> N; // number of Census tracts. \n",
    "  int<lower=0> N_edges; // number of edges in the graph (i.e. number of pairs of adjacent Census tracts). \n",
    "  array[N_edges] int<lower=1, upper=N> node1; // node1[i] adjacent to node2[i]\n",
    "  array[N_edges] int<lower=1, upper=N> node2; // and node1[i] < node2[i]\n",
    "  array[N] int<lower=0> n_images; // vector with one entry per Census tract of the number of images in that tract. \n",
    "  array[N] int<lower=0> n_classified_positive; // vector with one entry per Census tract of number of images classified positive. \n",
    "  int<lower=0,upper=1> use_ICAR_prior; // 1 if you want to use ICAR prior, 0 if you don't. ICAR prior basically smooths the data. \n",
    "  real <lower=0> ICAR_prior_weight; // weight of ICAR prior.\n",
    "\n",
    "  //annotation sample. \n",
    "  int n_annotated_classified_negative;\n",
    "  int n_annotated_classified_positive;\n",
    "  int n_annotated_classified_negative_true_positive;\n",
    "  int n_annotated_classified_positive_true_positive;  \n",
    "}\n",
    "parameters {\n",
    "  vector[N] phi;\n",
    "  real<upper=0> phi_offset; // this is the mean from which phis are drawn. Upper bound at 0 to rule out bad modes and set prior that true positives are rare. \n",
    "  ordered[2] logit_p_y_1_given_y_hat; // ordered to impose the constraint that p_y_1_given_y_hat_0 < p_y_1_given_y_hat_1.\n",
    "}\n",
    "transformed parameters {\n",
    "    real p_y_1_given_y_hat_0 = inv_logit(logit_p_y_1_given_y_hat[1]);\n",
    "    real p_y_1_given_y_hat_1 = inv_logit(logit_p_y_1_given_y_hat[2]);\n",
    "    vector[N] p_y = inv_logit(phi);\n",
    "    real empirical_p_yhat = sum(n_classified_positive) * 1.0 / sum(n_images);\n",
    "    real p_y_hat_1_given_y_1 = empirical_p_yhat * p_y_1_given_y_hat_1 / (empirical_p_yhat * p_y_1_given_y_hat_1 + (1 - empirical_p_yhat) * p_y_1_given_y_hat_0);\n",
    "    real p_y_hat_1_given_y_0 = empirical_p_yhat * (1 - p_y_1_given_y_hat_1) / (empirical_p_yhat * (1 - p_y_1_given_y_hat_1) + (1 - empirical_p_yhat) * (1 - p_y_1_given_y_hat_0));\n",
    "}\n",
    "model {\n",
    "  // You can't just scale ICAR priors by random numbers; the only principled value for ICAR_prior_weight is 0.5. \n",
    "  // https://stats.stackexchange.com/questions/333258/strength-parameter-in-icar-spatial-model\n",
    "  // still, there's no computational reason you can't use another value. \n",
    "  if (use_ICAR_prior == 1) {\n",
    "    target += -ICAR_prior_weight * dot_self(phi[node1] - phi[node2]);\n",
    "  }\n",
    "\n",
    "  // model the results on the annotation set. \n",
    "  n_annotated_classified_negative_true_positive ~ binomial(n_annotated_classified_negative, p_y_1_given_y_hat_0);\n",
    "  n_annotated_classified_positive_true_positive ~ binomial(n_annotated_classified_positive, p_y_1_given_y_hat_1);\n",
    "  \n",
    "  // model the results by Census tract. \n",
    "  phi_offset ~ normal(0, 2);\n",
    "  phi ~ normal(phi_offset, 1); \n",
    "  n_classified_positive ~ binomial(n_images, p_y .* p_y_hat_1_given_y_1 + (1 - p_y) .* p_y_hat_1_given_y_0);\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import expit\n",
    "\n",
    "def generate_real_data(df, \n",
    "                       n_annotated_classified_negative, \n",
    "                       n_annotated_classified_positive, \n",
    "                       icar_prior_setting, \n",
    "                       scaling_factor_positive=1.0, \n",
    "                       scaling_factor_negative=1.0):\n",
    "    \"\"\"\n",
    "    Generate data for the model using real data from a DataFrame, with scaling factors to modulate the influence \n",
    "    of annotated data.\n",
    "    \"\"\"\n",
    "    # HERE: changed this from 1000 \n",
    "    N = len(df)\n",
    "    total_images = df['total_images'].values\n",
    "    positive_images = df['positive_images'].values\n",
    "\n",
    "    # Generate adjacency matrix and neighborhood structure\n",
    "    node1 = []\n",
    "    node2 = []\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            if np.random.rand() < 0.1:\n",
    "                node1.append(i + 1)  # one indexing for Stan\n",
    "                node2.append(j + 1)\n",
    "\n",
    "    # ONLY PLUG IN OBSERVED DATA from the ct_dataset here\n",
    "    # WRITE UP 'how do i install stan' walkthrough in onboarding on G2 \n",
    "    \n",
    "\n",
    "    return {'observed_data': {\n",
    "                'N': N, 'N_edges': len(node1), 'node1': node1, 'node2': node2, \n",
    "                'n_images': df['total_images'].tolist(),\n",
    "                'n_classified_positive': df['positive_images'].tolist(), \n",
    "                'n_annotated_classified_negative': 500,\n",
    "                'n_annotated_classified_positive': 500,\n",
    "                'n_annotated_classified_negative_true_positive': 3,\n",
    "                'n_annotated_classified_positive_true_positive': 330\n",
    "            }\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_simulated_data(N, images_per_location, \n",
    "                            n_annotated_classified_negative, \n",
    "                            n_annotated_classified_positive, \n",
    "                            icar_prior_setting):\n",
    "    \"\"\"\n",
    "    Generate simulated data for the model.\n",
    "    \"\"\"    \n",
    "    node1 = []\n",
    "    node2 = []\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            if np.random.rand() < 0.1:\n",
    "                node1.append(i + 1) # one indexing for Stan. \n",
    "                node2.append(j + 1)\n",
    "    phi_offset = random.random() * -3 - 1 # mean of phi.\n",
    "\n",
    "    # these only matter for CAR model. https://mc-stan.org/users/documentation/case-studies/mbjoseph-CARStan.html\n",
    "    D = np.zeros((N, N))\n",
    "    W = np.zeros((N, N))\n",
    "    for i in range(len(node1)):\n",
    "        D[node1[i] - 1, node1[i] - 1] += 1\n",
    "        D[node2[i] - 1, node2[i] - 1] += 1\n",
    "        W[node1[i] - 1, node2[i] - 1] = 1\n",
    "        W[node2[i] - 1, node1[i] - 1] = 1\n",
    "    B = np.linalg.inv(D) @ W\n",
    "    tau = np.random.gamma(scale=0.2, shape=2)\n",
    "    alpha = np.random.random()\n",
    "    sigma = np.linalg.inv(tau * D @ (np.eye(N) - alpha * B))\n",
    "    if icar_prior_setting != 'none':    \n",
    "        phi = np.random.multivariate_normal(mean=np.zeros(N), cov=sigma)\n",
    "    else:\n",
    "        phi = np.random.normal(loc=0, size=N) # this uses no icar prior, just draws everything independently. \n",
    "    p_Y = expit(phi + phi_offset)\n",
    "    n_images = np.random.poisson(images_per_location, N)\n",
    "    p_y_hat_1_given_y_1 = random.random() * 0.5 + 0.2\n",
    "    p_y_hat_1_given_y_0 = random.random() * 0.01 + 0.01\n",
    "\n",
    "    n_classified_positive = []\n",
    "    n_true_positive = []\n",
    "    for i in range(N):\n",
    "        n_true_positive.append(np.random.binomial(n_images[i], p_Y[i]))\n",
    "        n_classified_positive.append(np.random.binomial(n_true_positive[-1], p_y_hat_1_given_y_1) + \n",
    "                                    np.random.binomial(n_images[i] - n_true_positive[-1], p_y_hat_1_given_y_0))\n",
    "    empirical_p_yhat = sum(n_classified_positive) * 1.0 / sum(n_images)\n",
    "    empirical_p_y = sum(n_true_positive) * 1.0 / sum(n_images)\n",
    "    p_y_1_given_y_hat_1 = p_y_hat_1_given_y_1 * empirical_p_y / empirical_p_yhat\n",
    "    p_y_1_given_y_hat_0 = (1 - p_y_hat_1_given_y_1) * empirical_p_y / (1 - empirical_p_yhat)\n",
    "    print(\"empirical_p_y\", empirical_p_y)\n",
    "    print(\"empirical_p_yhat\", empirical_p_yhat)\n",
    "    print(\"p_y_hat_1_given_y_1\", p_y_hat_1_given_y_1)\n",
    "    print(\"p_y_hat_1_given_y_0\", p_y_hat_1_given_y_0)\n",
    "    print(\"p_y_1_given_y_hat_1\", p_y_1_given_y_hat_1)\n",
    "    print(\"p_y_1_given_y_hat_0\", p_y_1_given_y_hat_0)\n",
    "                     \n",
    "    n_annotated_classified_negative_true_positive = np.random.binomial(n_annotated_classified_negative, p_y_1_given_y_hat_0)\n",
    "    n_annotated_classified_positive_true_positive = np.random.binomial(n_annotated_classified_positive, p_y_1_given_y_hat_1)\n",
    "    print(\"number of annotated classified negative which were positive: %i/%i\" % (n_annotated_classified_negative_true_positive, n_annotated_classified_negative))\n",
    "    print(\"number of annotated classified positive which were positive: %i/%i\" % (n_annotated_classified_positive_true_positive, n_annotated_classified_positive))\n",
    "    \n",
    "\n",
    "    return {'observed_data':{'N':N, 'N_edges':len(node1), 'node1':node1, 'node2':node2, \n",
    "                             'n_images':n_images, 'n_classified_positive':n_classified_positive, \n",
    "                             'n_annotated_classified_negative':n_annotated_classified_negative,\n",
    "                                'n_annotated_classified_positive':n_annotated_classified_positive,\n",
    "                                'n_annotated_classified_negative_true_positive':n_annotated_classified_negative_true_positive,\n",
    "                                'n_annotated_classified_positive_true_positive':n_annotated_classified_positive_true_positive},\n",
    "\n",
    "            'parameters':{'phi':phi, 'phi_offset':phi_offset, \n",
    "                          'p_y_1_given_y_hat_1':p_y_1_given_y_hat_1,\n",
    "                            'p_y_1_given_y_hat_0':p_y_1_given_y_hat_0, \n",
    "                            'p_y_hat_1_given_y_1':p_y_hat_1_given_y_1,\n",
    "                            'p_y_hat_1_given_y_0':p_y_hat_1_given_y_0, \n",
    "                            'p_Y':p_Y, \n",
    "                            'tau':tau, 'alpha':alpha, 'sigma':sigma}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter logit_p_y_1_given_y_hat has 2 priors.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/17300)\n",
      "Sampling:   0% (2/17300)\n",
      "Sampling:   0% (3/17300)\n",
      "Sampling:   0% (4/17300)\n",
      "Sampling:   1% (103/17300)\n",
      "Sampling:   1% (203/17300)\n",
      "Sampling:   2% (302/17300)\n",
      "Sampling:   2% (401/17300)\n",
      "Sampling:   3% (500/17300)\n",
      "Sampling:   3% (600/17300)\n",
      "Sampling:   4% (700/17300)\n",
      "Sampling:   5% (800/17300)\n",
      "Sampling:   5% (900/17300)\n",
      "Sampling:   6% (1000/17300)\n",
      "Sampling:   6% (1100/17300)\n",
      "Sampling:   7% (1200/17300)\n",
      "Sampling:   8% (1300/17300)\n",
      "Sampling:   8% (1400/17300)\n",
      "Sampling:   9% (1500/17300)\n",
      "Sampling:   9% (1600/17300)\n",
      "Sampling:  10% (1700/17300)\n",
      "Sampling:  10% (1800/17300)\n",
      "Sampling:  11% (1900/17300)\n",
      "Sampling:  12% (2000/17300)\n",
      "Sampling:  12% (2100/17300)\n",
      "Sampling:  13% (2200/17300)\n",
      "Sampling:  13% (2300/17300)\n",
      "Sampling:  14% (2400/17300)\n",
      "Sampling:  14% (2500/17300)\n",
      "Sampling:  15% (2600/17300)\n",
      "Sampling:  16% (2700/17300)\n",
      "Sampling:  16% (2800/17300)\n",
      "Sampling:  17% (2900/17300)\n",
      "Sampling:  17% (3000/17300)\n",
      "Sampling:  18% (3100/17300)\n",
      "Sampling:  18% (3200/17300)\n",
      "Sampling:  19% (3300/17300)\n",
      "Sampling:  20% (3400/17300)\n",
      "Sampling:  20% (3500/17300)\n",
      "Sampling:  21% (3600/17300)\n",
      "Sampling:  21% (3700/17300)\n",
      "Sampling:  22% (3800/17300)\n",
      "Sampling:  23% (3900/17300)\n",
      "Sampling:  23% (4000/17300)\n",
      "Sampling:  24% (4100/17300)\n",
      "Sampling:  24% (4200/17300)\n",
      "Sampling:  25% (4300/17300)\n",
      "Sampling:  25% (4400/17300)\n",
      "Sampling:  26% (4500/17300)\n",
      "Sampling:  27% (4600/17300)\n",
      "Sampling:  27% (4700/17300)\n",
      "Sampling:  28% (4800/17300)\n",
      "Sampling:  28% (4900/17300)\n",
      "Sampling:  29% (5000/17300)\n",
      "Sampling:  29% (5100/17300)\n",
      "Sampling:  30% (5200/17300)\n",
      "Sampling:  31% (5300/17300)\n",
      "Sampling:  31% (5400/17300)\n",
      "Sampling:  36% (6300/17300)\n",
      "Sampling:  43% (7500/17300)\n",
      "Sampling:  48% (8300/17300)\n",
      "Sampling:  62% (10725/17300)\n",
      "Sampling:  69% (11925/17300)\n",
      "Sampling:  77% (13325/17300)\n",
      "Sampling:  89% (15425/17300)\n",
      "Sampling:  91% (15725/17300)\n",
      "Sampling:  92% (15850/17300)\n",
      "Sampling:  94% (16250/17300)\n",
      "Sampling:  98% (16975/17300)\n",
      "Sampling:  99% (17175/17300)\n",
      "Sampling: 100% (17275/17300)\n",
      "Sampling: 100% (17300/17300)\n",
      "Sampling: 100% (17300/17300), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.000253 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 2.53 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 0.000229 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 2.29 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_pvo2ls84/model_4m6qj2jd.stan', line 46, column 2 to column 108)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.000404 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 4.04 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_pvo2ls84/model_4m6qj2jd.stan', line 46, column 2 to column 108)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.000335 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 3.35 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_pvo2ls84/model_4m6qj2jd.stan', line 46, column 2 to column 108)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
      "p_y_hat_1_given_y_1  0.474  0.132   0.250    0.750      0.018    0.013   \n",
      "p_y_hat_1_given_y_0  0.000  0.000   0.000    0.000      0.000    0.000   \n",
      "phi_offset          -6.635  0.291  -7.216   -6.149      0.042    0.030   \n",
      "p_y_1_given_y_hat_1  0.778  0.015   0.750    0.805      0.000    0.000   \n",
      "p_y_1_given_y_hat_0  0.002  0.001   0.000    0.003      0.000    0.000   \n",
      "empirical_p_yhat     0.002  0.000   0.002    0.002      0.000    0.000   \n",
      "\n",
      "                     ess_bulk  ess_tail  r_hat  \n",
      "p_y_hat_1_given_y_1      51.0     119.0    1.1  \n",
      "p_y_hat_1_given_y_0    8736.0    7605.0    1.0  \n",
      "phi_offset               51.0     119.0    1.1  \n",
      "p_y_1_given_y_hat_1    8610.0    7604.0    1.0  \n",
      "p_y_1_given_y_hat_0      51.0     120.0    1.1  \n",
      "empirical_p_yhat       9296.0    9296.0    NaN  \n"
     ]
    }
   ],
   "source": [
    "icar_prior_setting = 'none'\n",
    "assert icar_prior_setting in ['none', 'cheating', 'proper']\n",
    "\n",
    "# Annotated counts\n",
    "NUM_CLASSIFIED_POSITIVE_ANNOTATED_POSITIVE = 329\n",
    "NUM_CLASSIFIED_POSITIVE_ANNOTATED_NEGATIVE = 171 \n",
    "NUM_CLASSIFIED_NEGATIVE_ANNOTATED_POSITIVE = 3 \n",
    "NUM_CLASSIFIED_NEGATIVE_ANNOTATED_NEGATIVE = 497\n",
    "\n",
    "\n",
    "# Total counts in the dataset\n",
    "TOTAL_PRED_POSITIVE = 1465 \n",
    "TOTAL_PRED_NEGATIVE = 924747\n",
    "\n",
    "\n",
    "for i in range(1):\n",
    "    NUM_WARMUP = 2000\n",
    "    NUM_SAMPLES = len(ct_dataset)\n",
    "    N = NUM_SAMPLES\n",
    "    \n",
    "    # Use the scaled counts in the model\n",
    "    data = generate_real_data(ct_dataset,\n",
    "                                n_annotated_classified_negative=670,\n",
    "                                n_annotated_classified_positive=330,\n",
    "                                icar_prior_setting=icar_prior_setting)\n",
    "                                \n",
    "    \n",
    "    if icar_prior_setting == 'proper':\n",
    "        raise Exception(\"Haven't verified that this model actually works! Need to review it / check on simulated data.\")\n",
    "        W = np.zeros((N, N))\n",
    "        \n",
    "        for i in range(len(data['observed_data']['node1'])):\n",
    "            W[data['observed_data']['node1'][i] - 1, \n",
    "                                data['observed_data']['node2'][i] - 1] = 1\n",
    "            W[data['observed_data']['node2'][i] - 1, \n",
    "                                data['observed_data']['node1'][i] - 1] = 1\n",
    "        del data['observed_data']['node1']\n",
    "        del data['observed_data']['node2']\n",
    "        data['observed_data']['W'] = W\n",
    "        data['observed_data']['W_n'] = int(W.sum() / 2)\n",
    "        model = stan.build(stan_code_proper_car_prior, data=data['observed_data'])\n",
    "    elif icar_prior_setting == 'cheating':\n",
    "        data['observed_data']['use_ICAR_prior'] = 1\n",
    "        data['observed_data']['ICAR_prior_weight'] = 0.05\n",
    "        model = stan.build(stan_code_with_weighted_ICAR_prior, data=data['observed_data'])\n",
    "    elif icar_prior_setting == 'none':\n",
    "        data['observed_data']['use_ICAR_prior'] = 0\n",
    "        data['observed_data']['ICAR_prior_weight'] = 0\n",
    "        model = stan.build(stan_code_with_weighted_ICAR_prior, data=data['observed_data'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid icar_prior_options\", icar_prior_setting)\n",
    "    \n",
    "    fit = model.sample(num_chains=4, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)\n",
    "    print(az.summary(fit, var_names=['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', 'phi_offset', \n",
    "                                    'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "                                    'empirical_p_yhat']))\n",
    "\n",
    "    df = fit.to_frame()\n",
    "\n",
    "    inferred_p_y = [df[f'p_y.{i}'].mean() for i in range(1, N + 1)]\n",
    "    # changing this to compute p_y from observed data \n",
    "    #p_y = np.nan_to_num(np.array(data['observed_data']['n_classified_positive']) / np.array(data['observed_data']['n_images']),0)\n",
    "    #plt.scatter(p_y, inferred_p_y)\n",
    "    #plt.title(\"True vs. inferred p_Y, r = %.2f\" %\n",
    "            #pearsonr(p_y, inferred_p_y)[0])\n",
    "    #max_val = max(max(p_y), max(inferred_p_y))\n",
    "    #plt.xlabel(\"True p_Y\")\n",
    "    #plt.ylabel(\"Inferred p_Y\")\n",
    "    #plt.plot([0, max_val], [0, max_val], 'r--')\n",
    "    #plt.xlim([0, max_val])\n",
    "    #plt.ylim([0, max_val])\n",
    "    #plt.figure(figsize=[12, 3])\n",
    "    \n",
    "    if icar_prior_setting == 'proper':\n",
    "        param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset', 'alpha', 'tau']\n",
    "    else:\n",
    "        param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset']\n",
    "    \n",
    "    #for k in param_names:\n",
    "    #    plt.subplot(1, len(param_names), param_names.index(k) + 1)\n",
    "    #    plt.hist(df[k], bins=50, density=True)\n",
    "    #    plt.title(k)\n",
    "    #    plt.axvline(data['parameters'][k], color='red')\n",
    "    \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1807244/2890272086.py:1: RuntimeWarning: invalid value encountered in divide\n",
      "  np.nan_to_num(np.array(data['observed_data']['n_classified_positive']) / np.array(data['observed_data']['n_images']),0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.00261438, ..., 0.        , 0.        ,\n",
       "       0.008     ])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_prior_setting = 'none'\n",
    "assert icar_prior_setting in ['none', 'cheating', 'proper']\n",
    "for i in range(10):\n",
    "    NUM_WARMUP = 2000\n",
    "    NUM_SAMPLES = 1000\n",
    "    N = 1000\n",
    "    simulated_data = generate_simulated_data(N=N, \n",
    "                                                images_per_location=1000, \n",
    "                                                n_annotated_classified_negative=500, \n",
    "                                                n_annotated_classified_positive=500, \n",
    "                                                icar_prior_setting=icar_prior_setting)\n",
    "    if icar_prior_setting == 'proper':\n",
    "        raise Exception(\"Haven't verified that this model actually works! Need to review it / check on simulated data. No need to review the parts which are identical to the other model.\")\n",
    "        W = np.zeros((N, N))\n",
    "        \n",
    "        for i in range(len(simulated_data['observed_data']['node1'])):\n",
    "            W[simulated_data['observed_data']['node1'][i] - 1, \n",
    "                                simulated_data['observed_data']['node2'][i] - 1] = 1\n",
    "            W[simulated_data['observed_data']['node2'][i] - 1, \n",
    "                                simulated_data['observed_data']['node1'][i] - 1] = 1\n",
    "        del simulated_data['observed_data']['node1']\n",
    "        del simulated_data['observed_data']['node2']\n",
    "        simulated_data['observed_data']['W'] = W\n",
    "        simulated_data['observed_data']['W_n'] = int(W.sum() / 2)\n",
    "        model = stan.build(stan_code_proper_car_prior, data=simulated_data['observed_data'])\n",
    "    elif icar_prior_setting == 'cheating':\n",
    "        simulated_data['observed_data']['use_ICAR_prior'] = 1\n",
    "        simulated_data['observed_data']['ICAR_prior_weight'] = 0.05\n",
    "        model = stan.build(stan_code_with_weighted_ICAR_prior, data=simulated_data['observed_data'])\n",
    "    elif icar_prior_setting == 'none':\n",
    "        simulated_data['observed_data']['use_ICAR_prior'] = 0\n",
    "        simulated_data['observed_data']['ICAR_prior_weight'] = 0\n",
    "        model = stan.build(stan_code_with_weighted_ICAR_prior, data=simulated_data['observed_data'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid icar_prior_options\", icar_prior_setting)\n",
    "    fit = model.sample(num_chains=4, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)\n",
    "    print(az.summary(fit, var_names=['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', 'phi_offset', \n",
    "                                    'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "                                    'empirical_p_yhat']))\n",
    "\n",
    "    df = fit.to_frame()\n",
    "\n",
    "    inferred_p_y = [df[f'p_y.{i}'].mean() for i in range(1, N + 1)]\n",
    "    plt.scatter(simulated_data['parameters']['p_Y'], inferred_p_y)\n",
    "    plt.title(\"True vs. inferred p_Y, r = %.2f\" %\n",
    "            pearsonr(simulated_data['parameters']['p_Y'], inferred_p_y)[0])\n",
    "    max_val = max(max(simulated_data['parameters']['p_Y']), max(inferred_p_y))\n",
    "    plt.xlabel(\"True p_Y\")\n",
    "    plt.ylabel(\"Inferred p_Y\")\n",
    "    plt.plot([0, max_val], [0, max_val], 'r--')\n",
    "    plt.xlim([0, max_val])\n",
    "    plt.ylim([0, max_val])\n",
    "    plt.figure(figsize=[12, 3])\n",
    "    if icar_prior_setting == 'proper':\n",
    "        param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset', 'alpha', 'tau']\n",
    "    else:\n",
    "        param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset']\n",
    "    for k in param_names:\n",
    "        plt.subplot(1, len(param_names), param_names.index(k) + 1)\n",
    "        # histogram of posterior samples\n",
    "        plt.hist(df[k], bins=50, density=True)\n",
    "        plt.title(k)\n",
    "        plt.axvline(simulated_data['parameters'][k], color='red')\n",
    "    plt.show()\n",
    "    #az.plot_trace(fit, var_names=['sigma_Z'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below here is deprecated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stan code below \"works\" but cheats on the ICAR prior. Set variable use_ICAR_prior to 0 if you don't want to use one at all.  \n",
    "DEPRECATED_stan_code_cheating_ICAR_prior_with_soft_zero_constraint = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0> N_edges;\n",
    "  array[N_edges] int<lower=1, upper=N> node1; // node1[i] adjacent to node2[i]\n",
    "  array[N_edges] int<lower=1, upper=N> node2; // and node1[i] < node2[i]\n",
    "  array[N] int<lower=0> n_images; \n",
    "  array[N] int<lower=0> n_classified_positive; \n",
    "  int<lower=0,upper=1> use_ICAR_prior; // 1 if you want to use ICAR prior, 0 if you don't. ICAR prior basically smooths the data. \n",
    "  real <lower=0> ICAR_prior_weight; // weight of ICAR prior.\n",
    "\n",
    "  //annotation sample. \n",
    "  int n_annotated_classified_negative;\n",
    "  int n_annotated_classified_positive;\n",
    "  int n_annotated_classified_negative_true_positive;\n",
    "  int n_annotated_classified_positive_true_positive;  \n",
    "}\n",
    "parameters {\n",
    "  vector[N] phi;\n",
    "  real phi_offset; \n",
    "  real<lower=0, upper=1> p_y_1_given_y_hat_1; \n",
    "  real<lower=0, upper=1> p_y_1_given_y_hat_0;\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[N] p_y = inv_logit(phi + phi_offset);\n",
    "    //real predicted_overall_p_y = sum(n_images .* p_y) / sum(n_images);\n",
    "    real empirical_p_yhat = sum(n_classified_positive) * 1.0 / sum(n_images);\n",
    "    real p_y_hat_1_given_y_1 = empirical_p_yhat * p_y_1_given_y_hat_1 / (empirical_p_yhat * p_y_1_given_y_hat_1 + (1 - empirical_p_yhat) * p_y_1_given_y_hat_0);\n",
    "    real p_y_hat_1_given_y_0 = empirical_p_yhat * (1 - p_y_1_given_y_hat_1) / (empirical_p_yhat * (1 - p_y_1_given_y_hat_1) + (1 - empirical_p_yhat) * (1 - p_y_1_given_y_hat_0));\n",
    "}\n",
    "model {\n",
    "\n",
    "  // You can't just scale ICAR priors by random numbers, so this is \"cheating\". Still, maybe a good sanity check. \n",
    "  // https://stats.stackexchange.com/questions/333258/strength-parameter-in-icar-spatial-model\n",
    "  if (use_ICAR_prior == 1) {\n",
    "    target += -ICAR_prior_weight * dot_self(phi[node1] - phi[node2]);\n",
    "  }\n",
    "  n_annotated_classified_negative_true_positive ~ binomial(n_annotated_classified_negative, p_y_1_given_y_hat_0);\n",
    "  n_annotated_classified_positive_true_positive ~ binomial(n_annotated_classified_positive, p_y_1_given_y_hat_1);\n",
    "  \n",
    "  // soft sum-to-zero constraint on phi,\n",
    "  // equivalent to mean(phi) ~ normal(0,0.01)\n",
    "  phi_offset ~ normal(0, 2);\n",
    "  //sum(phi) ~ normal(0, 0.01 * N); // ZERO CENTERED.\n",
    "  n_classified_positive ~ binomial(n_images, p_y .* p_y_hat_1_given_y_1 + (1 - p_y) .* p_y_hat_1_given_y_0);\n",
    "}\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_stan_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
