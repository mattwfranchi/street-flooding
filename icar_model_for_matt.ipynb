{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import multiprocessing\n",
    "# only set fork if not already set\n",
    "if multiprocessing.get_start_method(allow_none=True) is None:\n",
    "    multiprocessing.set_start_method(\"fork\")\n",
    "import pandas as pd\n",
    "import stan\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matt Updates (8/25)\n",
    "- Got conda environment that runs this notebook installed successfully \n",
    "- Now, to plug in real data \n",
    "- Anyway to suppress some of this output? \n",
    "- tried to add some upweighting to account for the fact that the 1000-image sample is not representative of overall image counts, not sure if this was the right intuition? The r_hat goes down to 1.0 for all of the fields, but there's this 'metropolis' error that pops up like 20 times. \n",
    "- i think something definitely needs to be done? these values aren't correct (which is what happens when i put 670/330 in, the raw counts of annotated neg/pos)\n",
    "\n",
    "Working results that don't make sense \n",
    "- empirical_p_y 0.22638264951208253\n",
    "- empirical_p_yhat 0.059639051600278256\n",
    "- p_y_hat_1_given_y_1 0.21163348990040542\n",
    "- p_y_hat_1_given_y_0 0.015385530450161835\n",
    "- p_y_1_given_y_hat_1 0.8033352121400738\n",
    "- p_y_1_given_y_hat_0 0.1897914834156601\n",
    "- number of annotated classified negative which were positive: 132/670\n",
    "- number of annotated classified positive which were positive: 276/330"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CTLabel', 'BoroCode', 'BoroName', 'CT2020', 'BoroCT2020', 'CDEligibil',\n",
       "       'NTAName', 'NTA2020', 'CDTA2020', 'CDTANAME', 'GEOID', 'PUMA',\n",
       "       'Shape_Leng', 'Shape_Area', 'geometry', 'total_images',\n",
       "       'positive_images'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load actual reuslts \n",
    "ct_dataset = pd.read_csv(\"data/processed/flooding_ct_dataset.csv\")\n",
    "ct_dataset[['total_images','positive_images']] = ct_dataset[['total_images','positive_images']].astype(int)\n",
    "ct_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emma notes (8/22):\n",
    "\n",
    "In general, let's build complexity iteratively. Start by getting reasonable results without worrying about ICAR prior/smoothing. Then use standard ICAR prior (with weight 0.5). Then use full CAR (maybe). Data generation code is reviewed + model without proper CAR prior is reviewed. Another thing it might be nice to implement at some point is using the information about where the annotated images are (i.e., what Census tracts). Could incorporate this as a multinomial (potentially?) \n",
    "\n",
    "Model with simple L2 smoothing (or no smoothing at all - stan_code_with_weighted_ICAR_prior):\n",
    "\n",
    "1. Consistently recovers parameters for realistic parameter settings (with no smoothing)\n",
    "2. Reviewed Stan code and looks good. \n",
    "3. ALso implemented L2 regularization for adjacent Census tracts. This isn't actually the \"proper\" way to do it, but might be useful on real data. Haven't tested how this performs. \n",
    "\n",
    "Model wih full CAR prior (stan_code_proper_car_prior):\n",
    "\n",
    "1. Haven't reviewed this or verified it recovers correct params (recently; I think I did a while ago). When you do review, don't need to re-review all the Bayesian conditioning math; it should be pretty similar to the old code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stan code with CAR prior seems to work but hasn't been reviewed. \n",
    "# CAR prior is https://mc-stan.org/users/documentation/case-studies/mbjoseph-CARStan.html\n",
    "stan_code_proper_car_prior = '''\n",
    "functions {\n",
    "  /**\n",
    "  * Return the log probability of a proper conditional autoregressive (CAR) prior \n",
    "  * with a sparse representation for the adjacency matrix\n",
    "  *\n",
    "  * @param phi Vector containing the parameters with a CAR prior\n",
    "  * @param tau Precision parameter for the CAR prior (real)\n",
    "  * @param alpha Dependence (usually spatial) parameter for the CAR prior (real)\n",
    "  * @param W_sparse Sparse representation of adjacency matrix (int array)\n",
    "  * @param n Length of phi (int)\n",
    "  * @param W_n Number of adjacent pairs (int)\n",
    "  * @param D_sparse Number of neighbors for each location (vector)\n",
    "  * @param lambda Eigenvalues of D^{-1/2}*W*D^{-1/2} (vector)\n",
    "  *\n",
    "  * @return Log probability density of CAR prior up to additive constant\n",
    "  */\n",
    "  real sparse_car_lpdf(vector phi, real tau, real alpha,\n",
    "                       array[,] int W_sparse, vector D_sparse, vector lambda,\n",
    "                       int n, int W_n) {\n",
    "    row_vector[n] phit_D; // phi' * D\n",
    "    row_vector[n] phit_W; // phi' * W\n",
    "    vector[n] ldet_terms;\n",
    "    \n",
    "    phit_D = (phi .* D_sparse)';\n",
    "    phit_W = rep_row_vector(0, n);\n",
    "    for (i in 1 : W_n) {\n",
    "      phit_W[W_sparse[i, 1]] = phit_W[W_sparse[i, 1]] + phi[W_sparse[i, 2]];\n",
    "      phit_W[W_sparse[i, 2]] = phit_W[W_sparse[i, 2]] + phi[W_sparse[i, 1]];\n",
    "    }\n",
    "    \n",
    "    for (i in 1 : n) {\n",
    "      ldet_terms[i] = log1m(alpha * lambda[i]);\n",
    "    }\n",
    "    return 0.5\n",
    "           * (n * log(tau) + sum(ldet_terms)\n",
    "              - tau * (phit_D * phi - alpha * (phit_W * phi)));\n",
    "  }\n",
    "}\n",
    "\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0> N_edges;\n",
    "  matrix<lower=0, upper=1>[N, N] W; // adjacency matrix\n",
    "  int W_n; // number of adjacent region pairs\n",
    "  array[N] int<lower=0> n_images; \n",
    "  array[N] int<lower=0> n_classified_positive; \n",
    "\n",
    "  //annotation sample. \n",
    "  int n_annotated_classified_negative;\n",
    "  int n_annotated_classified_positive;\n",
    "  int n_annotated_classified_negative_true_positive;\n",
    "  int n_annotated_classified_positive_true_positive;  \n",
    "}\n",
    "transformed data {\n",
    "  array[W_n, 2] int W_sparse; // adjacency pairs\n",
    "  vector[N] D_sparse; // diagonal of D (number of neigbors for each site)\n",
    "  vector[N] lambda; // eigenvalues of invsqrtD * W * invsqrtD\n",
    "  \n",
    "  {\n",
    "    // generate sparse representation for W\n",
    "    int counter;\n",
    "    counter = 1;\n",
    "    // loop over upper triangular part of W to identify neighbor pairs\n",
    "    for (i in 1 : (N - 1)) {\n",
    "      for (j in (i + 1) : N) {\n",
    "        if (W[i, j] == 1) {\n",
    "          W_sparse[counter, 1] = i;\n",
    "          W_sparse[counter, 2] = j;\n",
    "          counter = counter + 1;\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  for (i in 1 : N) {\n",
    "    D_sparse[i] = sum(W[i]);\n",
    "  }\n",
    "  {\n",
    "    vector[N] invsqrtD;\n",
    "    for (i in 1 : N) {\n",
    "      invsqrtD[i] = 1 / sqrt(D_sparse[i]);\n",
    "    }\n",
    "    lambda = eigenvalues_sym(quad_form(W, diag_matrix(invsqrtD)));\n",
    "  }\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  vector[N] phi;\n",
    "  real<lower=0> tau;\n",
    "  real<lower=0, upper=1> alpha;\n",
    "  real <upper=0>phi_offset; // you may not want to place this constraint but it helps convergence with small numbers of samples\n",
    "  real<lower=0, upper=1> p_y_1_given_y_hat_1; \n",
    "  real<lower=0, upper=1> p_y_1_given_y_hat_0;\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[N] p_y = inv_logit(phi + phi_offset);\n",
    "    //real predicted_overall_p_y = sum(n_images .* p_y) / sum(n_images);\n",
    "    real empirical_p_yhat = sum(n_classified_positive) * 1.0 / sum(n_images);\n",
    "    real p_y_hat_1_given_y_1 = empirical_p_yhat * p_y_1_given_y_hat_1 / (empirical_p_yhat * p_y_1_given_y_hat_1 + (1 - empirical_p_yhat) * p_y_1_given_y_hat_0);\n",
    "    real p_y_hat_1_given_y_0 = empirical_p_yhat * (1 - p_y_1_given_y_hat_1) / (empirical_p_yhat * (1 - p_y_1_given_y_hat_1) + (1 - empirical_p_yhat) * (1 - p_y_1_given_y_hat_0));\n",
    "}\n",
    "model {\n",
    "  n_annotated_classified_negative_true_positive ~ binomial(n_annotated_classified_negative , p_y_1_given_y_hat_0);\n",
    "  n_annotated_classified_positive_true_positive ~ binomial(n_annotated_classified_positive, p_y_1_given_y_hat_1);\n",
    "  tau ~ gamma(2, 2);\n",
    "  phi ~ sparse_car(tau, alpha, W_sparse, D_sparse, lambda, N, W_n);\n",
    "  phi_offset ~ normal(-4, 0.5);\n",
    "  n_classified_positive ~ binomial(n_images, p_y .* p_y_hat_1_given_y_1 + (1 - p_y) .* p_y_hat_1_given_y_0);\n",
    "}\n",
    "'''\n",
    "\n",
    "# Stan code below works and has been reviewed but does not implement a proper ICAR prior. Set variable use_ICAR_prior to 0 if you don't want to use one at all. \n",
    "# I think it might also be principled to set the ICAR prior weight to 0.5, https://mc-stan.org/users/documentation/case-studies/icar_stan.html. \n",
    "stan_code_with_weighted_ICAR_prior = '''\n",
    "data {\n",
    "  int<lower=0> N; // number of Census tracts. \n",
    "  int<lower=0> N_edges; // number of edges in the graph (i.e. number of pairs of adjacent Census tracts). \n",
    "  array[N_edges] int<lower=1, upper=N> node1; // node1[i] adjacent to node2[i]\n",
    "  array[N_edges] int<lower=1, upper=N> node2; // and node1[i] < node2[i]\n",
    "  array[N] int<lower=0> n_images; // vector with one entry per Census tract of the number of images in that tract. \n",
    "  array[N] int<lower=0> n_classified_positive; // vector with one entry per Census tract of number of images classified positive. \n",
    "  int<lower=0,upper=1> use_ICAR_prior; // 1 if you want to use ICAR prior, 0 if you don't. ICAR prior basically smooths the data. \n",
    "  real <lower=0> ICAR_prior_weight; // weight of ICAR prior.\n",
    "\n",
    "  //annotation sample. \n",
    "  int n_annotated_classified_negative;\n",
    "  int n_annotated_classified_positive;\n",
    "  int n_annotated_classified_negative_true_positive;\n",
    "  int n_annotated_classified_positive_true_positive;  \n",
    "}\n",
    "parameters {\n",
    "  vector[N] phi;\n",
    "  real<upper=0> phi_offset; // this is the mean from which phis are drawn. Upper bound at 0 to rule out bad modes and set prior that true positives are rare. \n",
    "  ordered[2] logit_p_y_1_given_y_hat; // ordered to impose the constraint that p_y_1_given_y_hat_0 < p_y_1_given_y_hat_1.\n",
    "}\n",
    "transformed parameters {\n",
    "    real p_y_1_given_y_hat_0 = inv_logit(logit_p_y_1_given_y_hat[1]);\n",
    "    real p_y_1_given_y_hat_1 = inv_logit(logit_p_y_1_given_y_hat[2]);\n",
    "    vector[N] p_y = inv_logit(phi);\n",
    "    real empirical_p_yhat = sum(n_classified_positive) * 1.0 / sum(n_images);\n",
    "    real p_y_hat_1_given_y_1 = empirical_p_yhat * p_y_1_given_y_hat_1 / (empirical_p_yhat * p_y_1_given_y_hat_1 + (1 - empirical_p_yhat) * p_y_1_given_y_hat_0);\n",
    "    real p_y_hat_1_given_y_0 = empirical_p_yhat * (1 - p_y_1_given_y_hat_1) / (empirical_p_yhat * (1 - p_y_1_given_y_hat_1) + (1 - empirical_p_yhat) * (1 - p_y_1_given_y_hat_0));\n",
    "}\n",
    "model {\n",
    "  // You can't just scale ICAR priors by random numbers; the only principled value for ICAR_prior_weight is 0.5. \n",
    "  // https://stats.stackexchange.com/questions/333258/strength-parameter-in-icar-spatial-model\n",
    "  // still, there's no computational reason you can't use another value. \n",
    "  if (use_ICAR_prior == 1) {\n",
    "    target += -ICAR_prior_weight * dot_self(phi[node1] - phi[node2]);\n",
    "  }\n",
    "\n",
    "  // model the results on the annotation set. \n",
    "  n_annotated_classified_negative_true_positive ~ binomial(n_annotated_classified_negative, p_y_1_given_y_hat_0);\n",
    "  n_annotated_classified_positive_true_positive ~ binomial(n_annotated_classified_positive, p_y_1_given_y_hat_1);\n",
    "  \n",
    "  // model the results by Census tract. \n",
    "  phi_offset ~ normal(0, 2);\n",
    "  phi ~ normal(phi_offset, 1); \n",
    "  n_classified_positive ~ binomial(n_images, p_y .* p_y_hat_1_given_y_1 + (1 - p_y) .* p_y_hat_1_given_y_0);\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.special import expit\n",
    "\n",
    "def generate_real_data(df, \n",
    "                       n_annotated_classified_negative, \n",
    "                       n_annotated_classified_positive, \n",
    "                       icar_prior_setting, \n",
    "                       scaling_factor_positive=1.0, \n",
    "                       scaling_factor_negative=1.0):\n",
    "    \"\"\"\n",
    "    Generate data for the model using real data from a DataFrame, with scaling factors to modulate the influence \n",
    "    of annotated data.\n",
    "    \"\"\"\n",
    "    # HERE: changed this from 1000 \n",
    "    N = len(df)\n",
    "    total_images = df['total_images'].values\n",
    "    positive_images = df['positive_images'].values\n",
    "\n",
    "    # Generate adjacency matrix and neighborhood structure\n",
    "    node1 = []\n",
    "    node2 = []\n",
    "    for i in range(N):\n",
    "        for j in range(i + 1, N):\n",
    "            if np.random.rand() < 0.1:\n",
    "                node1.append(i + 1)  # one indexing for Stan\n",
    "                node2.append(j + 1)\n",
    "\n",
    "    phi_offset = random.random() * -3 - 1  # mean of phi.\n",
    "\n",
    "    # CAR model-specific structures\n",
    "    D = np.zeros((N, N))\n",
    "    W = np.zeros((N, N))\n",
    "    for i in range(len(node1)):\n",
    "        D[node1[i] - 1, node1[i] - 1] += 1\n",
    "        D[node2[i] - 1, node2[i] - 1] += 1\n",
    "        W[node1[i] - 1, node2[i] - 1] = 1\n",
    "        W[node2[i] - 1, node1[i] - 1] = 1\n",
    "    B = np.linalg.inv(D) @ W\n",
    "    tau = np.random.gamma(scale=0.2, shape=2)\n",
    "    alpha = np.random.random()\n",
    "    sigma = np.linalg.inv(tau * D @ (np.eye(N) - alpha * B))\n",
    "    \n",
    "    if icar_prior_setting != 'none':    \n",
    "        phi = np.random.multivariate_normal(mean=np.zeros(N), cov=sigma)\n",
    "    else:\n",
    "        phi = np.random.normal(loc=0, size=N)  # no ICAR prior, draws independently.\n",
    "\n",
    "    p_Y = expit(phi + phi_offset)\n",
    "    p_y_hat_1_given_y_1 = random.random() * 0.5 + 0.2\n",
    "    p_y_hat_1_given_y_0 = random.random() * 0.01 + 0.01\n",
    "\n",
    "    n_classified_positive = []\n",
    "    n_true_positive = []\n",
    "    for i in range(N):\n",
    "        n_true_positive.append(np.random.binomial(total_images[i], p_Y[i]))\n",
    "        n_classified_positive.append(np.random.binomial(n_true_positive[-1], p_y_hat_1_given_y_1) + \n",
    "                                     np.random.binomial(total_images[i] - n_true_positive[-1], p_y_hat_1_given_y_0))\n",
    "\n",
    "    empirical_p_yhat = sum(n_classified_positive) / sum(total_images)\n",
    "    empirical_p_y = sum(n_true_positive) / sum(total_images)\n",
    "    p_y_1_given_y_hat_1 = p_y_hat_1_given_y_1 * empirical_p_y / empirical_p_yhat\n",
    "    p_y_1_given_y_hat_0 = (1 - p_y_hat_1_given_y_1) * empirical_p_y / (1 - empirical_p_yhat)\n",
    "\n",
    "    # Apply scaling factors to adjust annotated counts\n",
    "    upweighted_n_annotated_classified_negative = int(n_annotated_classified_negative * scaling_factor_negative)\n",
    "    upweighted_n_annotated_classified_positive = int(n_annotated_classified_positive * scaling_factor_positive)\n",
    "\n",
    "    n_annotated_classified_negative_true_positive = np.random.binomial(upweighted_n_annotated_classified_negative, p_y_1_given_y_hat_0)\n",
    "    n_annotated_classified_positive_true_positive = np.random.binomial(upweighted_n_annotated_classified_positive, p_y_1_given_y_hat_1)\n",
    "    \n",
    "    print(\"empirical_p_y\", empirical_p_y)\n",
    "    print(\"empirical_p_yhat\", empirical_p_yhat)\n",
    "    print(\"p_y_hat_1_given_y_1\", p_y_hat_1_given_y_1)\n",
    "    print(\"p_y_hat_1_given_y_0\", p_y_hat_1_given_y_0)\n",
    "    print(\"p_y_1_given_y_hat_1\", p_y_1_given_y_hat_1)\n",
    "    print(\"p_y_1_given_y_hat_0\", p_y_1_given_y_hat_0)\n",
    "    \n",
    "    print(\"number of annotated classified negative which were positive: %i/%i\" % (n_annotated_classified_negative_true_positive, upweighted_n_annotated_classified_negative))\n",
    "    print(\"number of annotated classified positive which were positive: %i/%i\" % (n_annotated_classified_positive_true_positive, upweighted_n_annotated_classified_positive))\n",
    "\n",
    "    return {'observed_data': {\n",
    "                'N': N, 'N_edges': len(node1), 'node1': node1, 'node2': node2, \n",
    "                'n_images': total_images.tolist(), \n",
    "                'n_classified_positive': n_classified_positive, \n",
    "                'n_annotated_classified_negative': upweighted_n_annotated_classified_negative,\n",
    "                'n_annotated_classified_positive': upweighted_n_annotated_classified_positive,\n",
    "                'n_annotated_classified_negative_true_positive': n_annotated_classified_negative_true_positive,\n",
    "                'n_annotated_classified_positive_true_positive': n_annotated_classified_positive_true_positive\n",
    "            },\n",
    "            'parameters': {\n",
    "                'phi': phi, 'phi_offset': phi_offset, \n",
    "                'p_y_1_given_y_hat_1': p_y_1_given_y_hat_1,\n",
    "                'p_y_1_given_y_hat_0': p_y_1_given_y_hat_0, \n",
    "                'p_y_hat_1_given_y_1': p_y_hat_1_given_y_1,\n",
    "                'p_y_hat_1_given_y_0': p_y_hat_1_given_y_0, \n",
    "                'p_Y': p_Y, \n",
    "                'tau': tau, 'alpha': alpha, 'sigma': sigma\n",
    "            }\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empirical_p_y 0.23176566959438813\n",
      "empirical_p_yhat 0.13501650229385134\n",
      "p_y_hat_1_given_y_1 0.5366469374051568\n",
      "p_y_hat_1_given_y_0 0.013913696770978208\n",
      "p_y_1_given_y_hat_1 0.9211935924157617\n",
      "p_y_1_given_y_hat_0 0.12415188624487085\n",
      "number of annotated classified negative which were positive: 98/670\n",
      "number of annotated classified positive which were positive: 305/330\n",
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter logit_p_y_1_given_y_hat has 2 priors.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/17300)\n",
      "Sampling:   0% (2/17300)\n",
      "Sampling:   0% (3/17300)\n",
      "Sampling:   0% (4/17300)\n",
      "Sampling:   1% (103/17300)\n",
      "Sampling:   1% (202/17300)\n",
      "Sampling:   2% (301/17300)\n",
      "Sampling:   2% (401/17300)\n",
      "Sampling:   3% (501/17300)\n",
      "Sampling:   3% (601/17300)\n",
      "Sampling:   4% (701/17300)\n",
      "Sampling:   5% (800/17300)\n",
      "Sampling:   5% (900/17300)\n",
      "Sampling:   6% (1000/17300)\n",
      "Sampling:   6% (1100/17300)\n",
      "Sampling:   7% (1200/17300)\n",
      "Sampling:   8% (1300/17300)\n",
      "Sampling:   8% (1400/17300)\n",
      "Sampling:   9% (1500/17300)\n",
      "Sampling:   9% (1600/17300)\n",
      "Sampling:  10% (1700/17300)\n",
      "Sampling:  10% (1800/17300)\n",
      "Sampling:  11% (1900/17300)\n",
      "Sampling:  12% (2000/17300)\n",
      "Sampling:  12% (2100/17300)\n",
      "Sampling:  13% (2200/17300)\n",
      "Sampling:  13% (2300/17300)\n",
      "Sampling:  14% (2400/17300)\n",
      "Sampling:  14% (2500/17300)\n",
      "Sampling:  15% (2600/17300)\n",
      "Sampling:  16% (2700/17300)\n",
      "Sampling:  16% (2800/17300)\n",
      "Sampling:  17% (2900/17300)\n",
      "Sampling:  17% (3000/17300)\n",
      "Sampling:  18% (3100/17300)\n",
      "Sampling:  18% (3200/17300)\n",
      "Sampling:  19% (3300/17300)\n",
      "Sampling:  20% (3400/17300)\n",
      "Sampling:  20% (3500/17300)\n",
      "Sampling:  21% (3600/17300)\n",
      "Sampling:  21% (3700/17300)\n",
      "Sampling:  22% (3800/17300)\n",
      "Sampling:  23% (3900/17300)\n",
      "Sampling:  23% (4000/17300)\n",
      "Sampling:  24% (4100/17300)\n",
      "Sampling:  24% (4200/17300)\n",
      "Sampling:  25% (4300/17300)\n",
      "Sampling:  25% (4400/17300)\n",
      "Sampling:  26% (4500/17300)\n",
      "Sampling:  27% (4600/17300)\n",
      "Sampling:  27% (4700/17300)\n",
      "Sampling:  28% (4800/17300)\n",
      "Sampling:  28% (4900/17300)\n",
      "Sampling:  29% (5000/17300)\n",
      "Sampling:  29% (5100/17300)\n",
      "Sampling:  30% (5200/17300)\n",
      "Sampling:  31% (5300/17300)\n",
      "Sampling:  31% (5400/17300)\n",
      "Sampling:  32% (5500/17300)\n",
      "Sampling:  32% (5600/17300)\n",
      "Sampling:  33% (5700/17300)\n",
      "Sampling:  34% (5800/17300)\n",
      "Sampling:  34% (5900/17300)\n",
      "Sampling:  35% (6000/17300)\n",
      "Sampling:  35% (6100/17300)\n",
      "Sampling:  36% (6200/17300)\n",
      "Sampling:  36% (6300/17300)\n",
      "Sampling:  37% (6400/17300)\n",
      "Sampling:  38% (6500/17300)\n",
      "Sampling:  38% (6600/17300)\n",
      "Sampling:  39% (6700/17300)\n",
      "Sampling:  39% (6800/17300)\n",
      "Sampling:  51% (8900/17300)\n",
      "Sampling:  58% (10100/17300)\n",
      "Sampling:  76% (13225/17300)\n",
      "Sampling:  77% (13326/17300)\n",
      "Sampling:  79% (13751/17300)\n",
      "Sampling:  87% (14976/17300)\n",
      "Sampling:  88% (15175/17300)\n",
      "Sampling:  88% (15275/17300)\n",
      "Sampling:  89% (15375/17300)\n",
      "Sampling:  89% (15475/17300)\n",
      "Sampling:  90% (15575/17300)\n",
      "Sampling:  91% (15675/17300)\n",
      "Sampling:  91% (15775/17300)\n",
      "Sampling:  92% (15975/17300)\n",
      "Sampling:  93% (16075/17300)\n",
      "Sampling:  93% (16175/17300)\n",
      "Sampling:  94% (16275/17300)\n",
      "Sampling:  95% (16375/17300)\n",
      "Sampling:  95% (16475/17300)\n",
      "Sampling:  96% (16575/17300)\n",
      "Sampling:  96% (16675/17300)\n",
      "Sampling:  97% (16775/17300)\n",
      "Sampling:  98% (16875/17300)\n",
      "Sampling:  98% (16975/17300)\n",
      "Sampling:  99% (17075/17300)\n",
      "Sampling: 100% (17300/17300)"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception during call to services function: `OSError(122, 'Disk quota exceeded')`, traceback: `['  File \"/share/ju/conda_virtualenvs/stan/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\\n    result = coro.send(None)\\n             ^^^^^^^^^^^^^^^\\n', '  File \"/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/httpstan/services_stub.py\", line 158, in call\\n    httpstan.cache.dump_fit(b\"\".join(compressed_parts), fit_name)\\n', '  File \"/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/httpstan/cache.py\", line 110, in dump_fit\\n    with path.open(\"wb\") as fh:\\n']`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid icar_prior_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, icar_prior_setting)\n\u001b[0;32m---> 56\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_warmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_WARMUP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_SAMPLES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(az\u001b[38;5;241m.\u001b[39msummary(fit, var_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_y_hat_1_given_y_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_y_hat_1_given_y_0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mphi_offset\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     58\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_y_1_given_y_hat_1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_y_1_given_y_hat_0\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     59\u001b[0m                                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempirical_p_yhat\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     61\u001b[0m df \u001b[38;5;241m=\u001b[39m fit\u001b[38;5;241m.\u001b[39mto_frame()\n",
      "File \u001b[0;32m/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/stan/model.py:89\u001b[0m, in \u001b[0;36mModel.sample\u001b[0;34m(self, num_chains, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, num_chains\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m stan\u001b[38;5;241m.\u001b[39mfit\u001b[38;5;241m.\u001b[39mFit:\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Draw samples from the model.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m    Parameters in ``kwargs`` will be passed to the default sample function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhmc_nuts_diag_e_adapt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/stan/model.py:108\u001b[0m, in \u001b[0;36mModel.hmc_nuts_diag_e_adapt\u001b[0;34m(self, num_chains, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Draw samples from the model using ``stan::services::sample::hmc_nuts_diag_e_adapt``.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03mParameters in ``kwargs`` will be passed to the (Python wrapper of)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    107\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstan::services::sample::hmc_nuts_diag_e_adapt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_chains\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_chains\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/stan/model.py:313\u001b[0m, in \u001b[0;36mModel._create_fit\u001b[0;34m(self, function, num_chains, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/share/ju/conda_virtualenvs/stan/lib/python3.12/asyncio/futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/share/ju/conda_virtualenvs/stan/lib/python3.12/asyncio/tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/stan/model.py:236\u001b[0m, in \u001b[0;36mModel._create_fit.<locals>.go\u001b[0;34m()\u001b[0m\n\u001b[1;32m    234\u001b[0m         sampling_output\u001b[38;5;241m.\u001b[39mwrite_line(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<info>Sampling:</info> <error>Initialization failed.</error>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization failed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(message)\n\u001b[1;32m    238\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception during call to services function: `OSError(122, 'Disk quota exceeded')`, traceback: `['  File \"/share/ju/conda_virtualenvs/stan/lib/python3.12/asyncio/tasks.py\", line 314, in __step_run_and_handle_result\\n    result = coro.send(None)\\n             ^^^^^^^^^^^^^^^\\n', '  File \"/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/httpstan/services_stub.py\", line 158, in call\\n    httpstan.cache.dump_fit(b\"\".join(compressed_parts), fit_name)\\n', '  File \"/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/httpstan/cache.py\", line 110, in dump_fit\\n    with path.open(\"wb\") as fh:\\n']`"
     ]
    }
   ],
   "source": [
    "icar_prior_setting = 'none'\n",
    "assert icar_prior_setting in ['none', 'cheating', 'proper']\n",
    "\n",
    "# Annotated counts\n",
    "NUM_CLASSIFIED_POSITIVE_ANNOTATED_POSITIVE = 329\n",
    "NUM_CLASSIFIED_POSITIVE_ANNOTATED_NEGATIVE = 171 \n",
    "NUM_CLASSIFIED_NEGATIVE_ANNOTATED_POSITIVE = 3 \n",
    "NUM_CLASSIFIED_NEGATIVE_ANNOTATED_NEGATIVE = 497\n",
    "\n",
    "\n",
    "# Total counts in the dataset\n",
    "TOTAL_PRED_POSITIVE = 1465 \n",
    "TOTAL_PRED_NEGATIVE = 924747\n",
    "\n",
    "# Calculate weighting factors (as before)\n",
    "SCALING_FACTOR_POSITIVE = TOTAL_PRED_POSITIVE / (NUM_CLASSIFIED_POSITIVE_ANNOTATED_POSITIVE + NUM_CLASSIFIED_NEGATIVE_ANNOTATED_POSITIVE)\n",
    "SCALING_FACTOR_NEGATIVE = TOTAL_PRED_NEGATIVE / (NUM_CLASSIFIED_POSITIVE_ANNOTATED_NEGATIVE + NUM_CLASSIFIED_NEGATIVE_ANNOTATED_NEGATIVE)\n",
    "\n",
    "for i in range(10):\n",
    "    NUM_WARMUP = 2000\n",
    "    NUM_SAMPLES = len(ct_dataset)\n",
    "    N = NUM_SAMPLES\n",
    "    \n",
    "    # Use the scaled counts in the model\n",
    "    data = generate_real_data(ct_dataset,\n",
    "                                n_annotated_classified_negative=670,\n",
    "                                n_annotated_classified_positive=330,\n",
    "                                icar_prior_setting=icar_prior_setting)\n",
    "                                \n",
    "    \n",
    "    if icar_prior_setting == 'proper':\n",
    "        raise Exception(\"Haven't verified that this model actually works! Need to review it / check on simulated data.\")\n",
    "        W = np.zeros((N, N))\n",
    "        \n",
    "        for i in range(len(data['observed_data']['node1'])):\n",
    "            W[data['observed_data']['node1'][i] - 1, \n",
    "                                data['observed_data']['node2'][i] - 1] = 1\n",
    "            W[data['observed_data']['node2'][i] - 1, \n",
    "                                data['observed_data']['node1'][i] - 1] = 1\n",
    "        del data['observed_data']['node1']\n",
    "        del data['observed_data']['node2']\n",
    "        data['observed_data']['W'] = W\n",
    "        data['observed_data']['W_n'] = int(W.sum() / 2)\n",
    "        model = stan.build(stan_code_proper_car_prior, data=data['observed_data'])\n",
    "    elif icar_prior_setting == 'cheating':\n",
    "        data['observed_data']['use_ICAR_prior'] = 1\n",
    "        data['observed_data']['ICAR_prior_weight'] = 0.05\n",
    "        model = stan.build(stan_code_with_weighted_ICAR_prior, data=data['observed_data'])\n",
    "    elif icar_prior_setting == 'none':\n",
    "        data['observed_data']['use_ICAR_prior'] = 0\n",
    "        data['observed_data']['ICAR_prior_weight'] = 0\n",
    "        model = stan.build(stan_code_with_weighted_ICAR_prior, data=data['observed_data'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid icar_prior_options\", icar_prior_setting)\n",
    "    \n",
    "    fit = model.sample(num_chains=4, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)\n",
    "    print(az.summary(fit, var_names=['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', 'phi_offset', \n",
    "                                    'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "                                    'empirical_p_yhat']))\n",
    "\n",
    "    df = fit.to_frame()\n",
    "\n",
    "    inferred_p_y = [df[f'p_y.{i}'].mean() for i in range(1, N + 1)]\n",
    "    plt.scatter(data['parameters']['p_Y'], inferred_p_y)\n",
    "    plt.title(\"True vs. inferred p_Y, r = %.2f\" %\n",
    "            pearsonr(data['parameters']['p_Y'], inferred_p_y)[0])\n",
    "    max_val = max(max(data['parameters']['p_Y']), max(inferred_p_y))\n",
    "    plt.xlabel(\"True p_Y\")\n",
    "    plt.ylabel(\"Inferred p_Y\")\n",
    "    plt.plot([0, max_val], [0, max_val], 'r--')\n",
    "    plt.xlim([0, max_val])\n",
    "    plt.ylim([0, max_val])\n",
    "    plt.figure(figsize=[12, 3])\n",
    "    \n",
    "    if icar_prior_setting == 'proper':\n",
    "        param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset', 'alpha', 'tau']\n",
    "    else:\n",
    "        param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset']\n",
    "    \n",
    "    for k in param_names:\n",
    "        plt.subplot(1, len(param_names), param_names.index(k) + 1)\n",
    "        plt.hist(df[k], bins=50, density=True)\n",
    "        plt.title(k)\n",
    "        plt.axvline(data['parameters'][k], color='red')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inferred_p_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2325,)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['parameters']['p_Y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_prior_setting = 'none'\n",
    "assert icar_prior_setting in ['none', 'cheating', 'proper']\n",
    "for i in range(10):\n",
    "    NUM_WARMUP = 2000\n",
    "    NUM_SAMPLES = 1000\n",
    "    N = 1000\n",
    "    simulated_data = generate_simulated_data(N=N, \n",
    "                                                images_per_location=1000, \n",
    "                                                n_annotated_classified_negative=500, \n",
    "                                                n_annotated_classified_positive=500, \n",
    "                                                icar_prior_setting=icar_prior_setting)\n",
    "    if icar_prior_setting == 'proper':\n",
    "        raise Exception(\"Haven't verified that this model actually works! Need to review it / check on simulated data. No need to review the parts which are identical to the other model.\")\n",
    "        W = np.zeros((N, N))\n",
    "        \n",
    "        for i in range(len(simulated_data['observed_data']['node1'])):\n",
    "            W[simulated_data['observed_data']['node1'][i] - 1, \n",
    "                                simulated_data['observed_data']['node2'][i] - 1] = 1\n",
    "            W[simulated_data['observed_data']['node2'][i] - 1, \n",
    "                                simulated_data['observed_data']['node1'][i] - 1] = 1\n",
    "        del simulated_data['observed_data']['node1']\n",
    "        del simulated_data['observed_data']['node2']\n",
    "        simulated_data['observed_data']['W'] = W\n",
    "        simulated_data['observed_data']['W_n'] = int(W.sum() / 2)\n",
    "        model = stan.build(stan_code_proper_car_prior, data=simulated_data['observed_data'])\n",
    "    elif icar_prior_setting == 'cheating':\n",
    "        simulated_data['observed_data']['use_ICAR_prior'] = 1\n",
    "        simulated_data['observed_data']['ICAR_prior_weight'] = 0.05\n",
    "        model = stan.build(stan_code_with_weighted_ICAR_prior, data=simulated_data['observed_data'])\n",
    "    elif icar_prior_setting == 'none':\n",
    "        simulated_data['observed_data']['use_ICAR_prior'] = 0\n",
    "        simulated_data['observed_data']['ICAR_prior_weight'] = 0\n",
    "        model = stan.build(stan_code_with_weighted_ICAR_prior, data=simulated_data['observed_data'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid icar_prior_options\", icar_prior_setting)\n",
    "    fit = model.sample(num_chains=4, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)\n",
    "    print(az.summary(fit, var_names=['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', 'phi_offset', \n",
    "                                    'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "                                    'empirical_p_yhat']))\n",
    "\n",
    "    df = fit.to_frame()\n",
    "\n",
    "    inferred_p_y = [df[f'p_y.{i}'].mean() for i in range(1, N + 1)]\n",
    "    plt.scatter(simulated_data['parameters']['p_Y'], inferred_p_y)\n",
    "    plt.title(\"True vs. inferred p_Y, r = %.2f\" %\n",
    "            pearsonr(simulated_data['parameters']['p_Y'], inferred_p_y)[0])\n",
    "    max_val = max(max(simulated_data['parameters']['p_Y']), max(inferred_p_y))\n",
    "    plt.xlabel(\"True p_Y\")\n",
    "    plt.ylabel(\"Inferred p_Y\")\n",
    "    plt.plot([0, max_val], [0, max_val], 'r--')\n",
    "    plt.xlim([0, max_val])\n",
    "    plt.ylim([0, max_val])\n",
    "    plt.figure(figsize=[12, 3])\n",
    "    if icar_prior_setting == 'proper':\n",
    "        param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset', 'alpha', 'tau']\n",
    "    else:\n",
    "        param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset']\n",
    "    for k in param_names:\n",
    "        plt.subplot(1, len(param_names), param_names.index(k) + 1)\n",
    "        # histogram of posterior samples\n",
    "        plt.hist(df[k], bins=50, density=True)\n",
    "        plt.title(k)\n",
    "        plt.axvline(simulated_data['parameters'][k], color='red')\n",
    "    plt.show()\n",
    "    #az.plot_trace(fit, var_names=['sigma_Z'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below here is deprecated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stan code below \"works\" but cheats on the ICAR prior. Set variable use_ICAR_prior to 0 if you don't want to use one at all.  \n",
    "DEPRECATED_stan_code_cheating_ICAR_prior_with_soft_zero_constraint = '''\n",
    "data {\n",
    "  int<lower=0> N;\n",
    "  int<lower=0> N_edges;\n",
    "  array[N_edges] int<lower=1, upper=N> node1; // node1[i] adjacent to node2[i]\n",
    "  array[N_edges] int<lower=1, upper=N> node2; // and node1[i] < node2[i]\n",
    "  array[N] int<lower=0> n_images; \n",
    "  array[N] int<lower=0> n_classified_positive; \n",
    "  int<lower=0,upper=1> use_ICAR_prior; // 1 if you want to use ICAR prior, 0 if you don't. ICAR prior basically smooths the data. \n",
    "  real <lower=0> ICAR_prior_weight; // weight of ICAR prior.\n",
    "\n",
    "  //annotation sample. \n",
    "  int n_annotated_classified_negative;\n",
    "  int n_annotated_classified_positive;\n",
    "  int n_annotated_classified_negative_true_positive;\n",
    "  int n_annotated_classified_positive_true_positive;  \n",
    "}\n",
    "parameters {\n",
    "  vector[N] phi;\n",
    "  real phi_offset; \n",
    "  real<lower=0, upper=1> p_y_1_given_y_hat_1; \n",
    "  real<lower=0, upper=1> p_y_1_given_y_hat_0;\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[N] p_y = inv_logit(phi + phi_offset);\n",
    "    //real predicted_overall_p_y = sum(n_images .* p_y) / sum(n_images);\n",
    "    real empirical_p_yhat = sum(n_classified_positive) * 1.0 / sum(n_images);\n",
    "    real p_y_hat_1_given_y_1 = empirical_p_yhat * p_y_1_given_y_hat_1 / (empirical_p_yhat * p_y_1_given_y_hat_1 + (1 - empirical_p_yhat) * p_y_1_given_y_hat_0);\n",
    "    real p_y_hat_1_given_y_0 = empirical_p_yhat * (1 - p_y_1_given_y_hat_1) / (empirical_p_yhat * (1 - p_y_1_given_y_hat_1) + (1 - empirical_p_yhat) * (1 - p_y_1_given_y_hat_0));\n",
    "}\n",
    "model {\n",
    "\n",
    "  // You can't just scale ICAR priors by random numbers, so this is \"cheating\". Still, maybe a good sanity check. \n",
    "  // https://stats.stackexchange.com/questions/333258/strength-parameter-in-icar-spatial-model\n",
    "  if (use_ICAR_prior == 1) {\n",
    "    target += -ICAR_prior_weight * dot_self(phi[node1] - phi[node2]);\n",
    "  }\n",
    "  n_annotated_classified_negative_true_positive ~ binomial(n_annotated_classified_negative, p_y_1_given_y_hat_0);\n",
    "  n_annotated_classified_positive_true_positive ~ binomial(n_annotated_classified_positive, p_y_1_given_y_hat_1);\n",
    "  \n",
    "  // soft sum-to-zero constraint on phi,\n",
    "  // equivalent to mean(phi) ~ normal(0,0.01)\n",
    "  phi_offset ~ normal(0, 2);\n",
    "  //sum(phi) ~ normal(0, 0.01 * N); // ZERO CENTERED.\n",
    "  n_classified_positive ~ binomial(n_images, p_y .* p_y_hat_1_given_y_1 + (1 - p_y) .* p_y_hat_1_given_y_0);\n",
    "}\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_stan_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
