{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import util\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import multiprocessing\n",
    "# if context not already set \n",
    "if multiprocessing.get_start_method(allow_none=True) is None:\n",
    "    multiprocessing.set_start_method(\"fork\")\n",
    "import pandas as pd\n",
    "import stan\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from scipy.special import expit\n",
    "import matplotlib.pyplot as plt\n",
    "import arviz as az\n",
    "import random\n",
    "\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emma notes (8/22):\n",
    "\n",
    "In general, let's build complexity iteratively. Start by getting reasonable results without worrying about ICAR prior/smoothing. Then use standard ICAR prior (with weight 0.5). Then use full CAR (maybe). Data generation code is reviewed + model without proper CAR prior is reviewed. Another thing it might be nice to implement at some point is using the information about where the annotated images are (i.e., what Census tracts). Could incorporate this as a multinomial (potentially?) \n",
    "\n",
    "Model with simple L2 smoothing (or no smoothing at all - stan_code_with_weighted_ICAR_prior):\n",
    "\n",
    "1. Consistently recovers parameters for realistic parameter settings (with no smoothing)\n",
    "2. Reviewed Stan code and looks good. \n",
    "3. ALso implemented L2 regularization for adjacent Census tracts. This isn't actually the \"proper\" way to do it, but might be useful on real data. Haven't tested how this performs. \n",
    "\n",
    "Model wih full CAR prior (stan_code_proper_car_prior):\n",
    "\n",
    "1. Haven't reviewed this or verified it recovers correct params (recently; I think I did a while ago). When you do review, don't need to re-review all the Bayesian conditioning math; it should be pretty similar to the old code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['N', 'N_edges', 'node1', 'node2', 'n_images_by_area', 'n_classified_positive_by_area', 'n_classified_positive_annotated_positive_by_area', 'n_classified_positive_annotated_negative_by_area', 'n_classified_negative_annotated_negative_by_area', 'n_classified_negative_annotated_positive_by_area', 'n_non_annotated_by_area', 'n_non_annotated_by_area_classified_positive', 'use_ICAR_prior', 'ICAR_prior_weight'])\n",
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter logit_p_y_1_given_y_hat has no priors. This means\n",
      "    either no prior is provided, or the prior(s) depend on data variables. In\n",
      "    the later case, this may be a false positive.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/10000)\n",
      "Sampling:   0% (2/10000)\n",
      "Sampling:   0% (3/10000)\n",
      "Sampling:   0% (4/10000)\n",
      "Sampling:   1% (103/10000)\n",
      "Sampling:   2% (203/10000)\n",
      "Sampling:   3% (303/10000)\n",
      "Sampling:   4% (403/10000)\n",
      "Sampling:   5% (502/10000)\n",
      "Sampling:   6% (602/10000)\n",
      "Sampling:   7% (701/10000)\n",
      "Sampling:   8% (800/10000)\n",
      "Sampling:   9% (900/10000)\n",
      "Sampling:  10% (1000/10000)\n",
      "Sampling:  11% (1100/10000)\n",
      "Sampling:  12% (1200/10000)\n",
      "Sampling:  13% (1300/10000)\n",
      "Sampling:  14% (1400/10000)\n",
      "Sampling:  15% (1500/10000)\n",
      "Sampling:  16% (1600/10000)\n",
      "Sampling:  17% (1700/10000)\n",
      "Sampling:  18% (1800/10000)\n",
      "Sampling:  19% (1900/10000)\n",
      "Sampling:  20% (2000/10000)\n",
      "Sampling:  21% (2100/10000)\n",
      "Sampling:  22% (2200/10000)\n",
      "Sampling:  23% (2301/10000)\n",
      "Sampling:  24% (2401/10000)\n",
      "Sampling:  25% (2501/10000)\n",
      "Sampling:  26% (2601/10000)\n",
      "Sampling:  27% (2701/10000)\n",
      "Sampling:  28% (2801/10000)\n",
      "Sampling:  29% (2901/10000)\n",
      "Sampling:  30% (3001/10000)\n",
      "Sampling:  31% (3101/10000)\n",
      "Sampling:  32% (3201/10000)\n",
      "Sampling:  33% (3301/10000)\n",
      "Sampling:  34% (3401/10000)\n",
      "Sampling:  35% (3500/10000)\n",
      "Sampling:  36% (3601/10000)\n",
      "Sampling:  37% (3701/10000)\n",
      "Sampling:  38% (3801/10000)\n",
      "Sampling:  39% (3902/10000)\n",
      "Sampling:  40% (4002/10000)\n",
      "Sampling:  41% (4103/10000)\n",
      "Sampling:  42% (4203/10000)\n",
      "Sampling:  43% (4302/10000)\n",
      "Sampling:  44% (4401/10000)\n",
      "Sampling:  45% (4500/10000)\n",
      "Sampling:  46% (4600/10000)\n",
      "Sampling:  47% (4700/10000)\n",
      "Sampling:  48% (4800/10000)\n",
      "Sampling:  49% (4900/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  51% (5100/10000)\n",
      "Sampling:  52% (5200/10000)\n",
      "Sampling:  53% (5300/10000)\n",
      "Sampling:  54% (5400/10000)\n",
      "Sampling:  55% (5500/10000)\n",
      "Sampling:  56% (5600/10000)\n",
      "Sampling:  57% (5700/10000)\n",
      "Sampling:  58% (5800/10000)\n",
      "Sampling:  59% (5900/10000)\n",
      "Sampling:  60% (6000/10000)\n",
      "Sampling:  61% (6100/10000)\n",
      "Sampling:  62% (6200/10000)\n",
      "Sampling:  63% (6300/10000)\n",
      "Sampling:  64% (6400/10000)\n",
      "Sampling:  65% (6500/10000)\n",
      "Sampling:  66% (6600/10000)\n",
      "Sampling:  67% (6700/10000)\n",
      "Sampling:  68% (6800/10000)\n",
      "Sampling:  69% (6900/10000)\n",
      "Sampling:  70% (7000/10000)\n",
      "Sampling:  71% (7100/10000)\n",
      "Sampling:  72% (7200/10000)\n",
      "Sampling:  73% (7300/10000)\n",
      "Sampling:  74% (7400/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling:  76% (7600/10000)\n",
      "Sampling:  77% (7700/10000)\n",
      "Sampling:  78% (7800/10000)\n",
      "Sampling:  79% (7900/10000)\n",
      "Sampling:  80% (8000/10000)\n",
      "Sampling:  81% (8100/10000)\n",
      "Sampling:  82% (8200/10000)\n",
      "Sampling:  83% (8300/10000)\n",
      "Sampling:  84% (8400/10000)\n",
      "Sampling:  85% (8500/10000)\n",
      "Sampling:  86% (8600/10000)\n",
      "Sampling:  87% (8700/10000)\n",
      "Sampling:  88% (8800/10000)\n",
      "Sampling:  89% (8900/10000)\n",
      "Sampling:  90% (9000/10000)\n",
      "Sampling:  91% (9100/10000)\n",
      "Sampling:  92% (9200/10000)\n",
      "Sampling:  93% (9300/10000)\n",
      "Sampling:  94% (9400/10000)\n",
      "Sampling:  95% (9500/10000)\n",
      "Sampling:  96% (9600/10000)\n",
      "Sampling:  97% (9700/10000)\n",
      "Sampling:  98% (9800/10000)\n",
      "Sampling:  99% (9900/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.002466 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 24.66 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.002835 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 28.35 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.002944 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 29.44 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.002862 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 28.62 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
      "p_y_hat_1_given_y_1  0.018  0.002   0.016    0.021      0.000    0.000   \n",
      "p_y_hat_1_given_y_0  0.000  0.000   0.000    0.000      0.000    0.000   \n",
      "phi_offset          -3.076  0.100  -3.258   -2.887      0.005    0.004   \n",
      "p_y_1_given_y_hat_1  0.996  0.003   0.991    0.999      0.000    0.000   \n",
      "p_y_1_given_y_hat_0  0.085  0.007   0.072    0.098      0.000    0.000   \n",
      "...                    ...    ...     ...      ...        ...      ...   \n",
      "p_y[2322]            0.060  0.058   0.001    0.166      0.001    0.001   \n",
      "p_y[2323]            0.100  0.080   0.005    0.248      0.001    0.001   \n",
      "p_y[2324]            0.030  0.024   0.002    0.073      0.000    0.000   \n",
      "p_y[2325]            0.051  0.049   0.002    0.136      0.001    0.001   \n",
      "p_y[2326]            0.107  0.085   0.005    0.268      0.001    0.001   \n",
      "\n",
      "                     ess_bulk  ess_tail  r_hat  \n",
      "p_y_hat_1_given_y_1     431.0    1141.0   1.00  \n",
      "p_y_hat_1_given_y_0   13972.0    3569.0   1.00  \n",
      "phi_offset              387.0     970.0   1.01  \n",
      "p_y_1_given_y_hat_1   13901.0    3500.0   1.00  \n",
      "p_y_1_given_y_hat_0     431.0    1142.0   1.00  \n",
      "...                       ...       ...    ...  \n",
      "p_y[2322]             12439.0    3628.0   1.00  \n",
      "p_y[2323]             12376.0    3569.0   1.00  \n",
      "p_y[2324]             10064.0    3640.0   1.00  \n",
      "p_y[2325]             11455.0    3738.0   1.00  \n",
      "p_y[2326]             12142.0    4452.0   1.00  \n",
      "\n",
      "[2333 rows x 9 columns]\n",
      "Warning: 8 of 2327 empirical p_yhat values are 0; these are being ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3217425/1006087040.py:132: RuntimeWarning: invalid value encountered in divide\n",
      "  empirical_p_yhat = data_to_use['observed_data']['n_classified_positive_by_area'] / data_to_use['observed_data']['n_images_by_area']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['N', 'N_edges', 'node1', 'node2', 'n_images_by_area', 'n_classified_positive_by_area', 'n_classified_positive_annotated_positive_by_area', 'n_classified_positive_annotated_negative_by_area', 'n_classified_negative_annotated_negative_by_area', 'n_classified_negative_annotated_positive_by_area', 'n_non_annotated_by_area', 'n_non_annotated_by_area_classified_positive', 'use_ICAR_prior', 'ICAR_prior_weight'])\n",
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter logit_p_y_1_given_y_hat has no priors. This means\n",
      "    either no prior is provided, or the prior(s) depend on data variables. In\n",
      "    the later case, this may be a false positive.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/10000)\n",
      "Sampling:   0% (2/10000)\n",
      "Sampling:   0% (3/10000)\n",
      "Sampling:   0% (4/10000)\n",
      "Sampling:   1% (103/10000)\n",
      "Sampling:   2% (203/10000)\n",
      "Sampling:   3% (303/10000)\n",
      "Sampling:   4% (403/10000)\n",
      "Sampling:   5% (502/10000)\n",
      "Sampling:   6% (602/10000)\n",
      "Sampling:   7% (701/10000)\n",
      "Sampling:   8% (800/10000)\n",
      "Sampling:   9% (900/10000)\n",
      "Sampling:  10% (1000/10000)\n",
      "Sampling:  11% (1100/10000)\n",
      "Sampling:  12% (1200/10000)\n",
      "Sampling:  13% (1300/10000)\n",
      "Sampling:  14% (1400/10000)\n",
      "Sampling:  15% (1500/10000)\n",
      "Sampling:  16% (1600/10000)\n",
      "Sampling:  17% (1700/10000)\n",
      "Sampling:  18% (1800/10000)\n",
      "Sampling:  19% (1900/10000)\n",
      "Sampling:  20% (2000/10000)\n",
      "Sampling:  21% (2100/10000)\n",
      "Sampling:  22% (2200/10000)\n",
      "Sampling:  23% (2301/10000)\n",
      "Sampling:  24% (2401/10000)\n",
      "Sampling:  25% (2501/10000)\n",
      "Sampling:  26% (2601/10000)\n",
      "Sampling:  27% (2701/10000)\n",
      "Sampling:  28% (2801/10000)\n",
      "Sampling:  29% (2901/10000)\n",
      "Sampling:  30% (3001/10000)\n",
      "Sampling:  31% (3101/10000)\n",
      "Sampling:  32% (3201/10000)\n",
      "Sampling:  33% (3301/10000)\n",
      "Sampling:  34% (3401/10000)\n",
      "Sampling:  35% (3500/10000)\n",
      "Sampling:  36% (3601/10000)\n",
      "Sampling:  37% (3701/10000)\n",
      "Sampling:  38% (3801/10000)\n",
      "Sampling:  39% (3901/10000)\n",
      "Sampling:  40% (4002/10000)\n",
      "Sampling:  41% (4103/10000)\n",
      "Sampling:  42% (4203/10000)\n",
      "Sampling:  43% (4302/10000)\n",
      "Sampling:  44% (4401/10000)\n",
      "Sampling:  45% (4500/10000)\n",
      "Sampling:  46% (4600/10000)\n",
      "Sampling:  47% (4700/10000)\n",
      "Sampling:  48% (4800/10000)\n",
      "Sampling:  49% (4900/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  51% (5100/10000)\n",
      "Sampling:  52% (5200/10000)\n",
      "Sampling:  53% (5300/10000)\n",
      "Sampling:  54% (5400/10000)\n",
      "Sampling:  55% (5500/10000)\n",
      "Sampling:  56% (5600/10000)\n",
      "Sampling:  57% (5700/10000)\n",
      "Sampling:  58% (5800/10000)\n",
      "Sampling:  59% (5900/10000)\n",
      "Sampling:  60% (6000/10000)\n",
      "Sampling:  61% (6100/10000)\n",
      "Sampling:  62% (6200/10000)\n",
      "Sampling:  63% (6300/10000)\n",
      "Sampling:  64% (6400/10000)\n",
      "Sampling:  65% (6500/10000)\n",
      "Sampling:  66% (6600/10000)\n",
      "Sampling:  67% (6700/10000)\n",
      "Sampling:  68% (6800/10000)\n",
      "Sampling:  69% (6900/10000)\n",
      "Sampling:  70% (7000/10000)\n",
      "Sampling:  71% (7100/10000)\n",
      "Sampling:  72% (7200/10000)\n",
      "Sampling:  73% (7300/10000)\n",
      "Sampling:  74% (7400/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling:  76% (7600/10000)\n",
      "Sampling:  77% (7700/10000)\n",
      "Sampling:  78% (7800/10000)\n",
      "Sampling:  79% (7900/10000)\n",
      "Sampling:  80% (8000/10000)\n",
      "Sampling:  81% (8100/10000)\n",
      "Sampling:  82% (8200/10000)\n",
      "Sampling:  83% (8300/10000)\n",
      "Sampling:  84% (8400/10000)\n",
      "Sampling:  85% (8500/10000)\n",
      "Sampling:  86% (8600/10000)\n",
      "Sampling:  87% (8700/10000)\n",
      "Sampling:  88% (8800/10000)\n",
      "Sampling:  89% (8900/10000)\n",
      "Sampling:  90% (9000/10000)\n",
      "Sampling:  91% (9100/10000)\n",
      "Sampling:  92% (9200/10000)\n",
      "Sampling:  93% (9300/10000)\n",
      "Sampling:  94% (9400/10000)\n",
      "Sampling:  95% (9500/10000)\n",
      "Sampling:  96% (9600/10000)\n",
      "Sampling:  97% (9700/10000)\n",
      "Sampling:  98% (9800/10000)\n",
      "Sampling:  99% (9900/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.0029 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 29 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.0029 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 29 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.002965 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 29.65 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.002826 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 28.26 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
      "p_y_hat_1_given_y_1  0.018  0.002   0.016    0.021      0.000    0.000   \n",
      "p_y_hat_1_given_y_0  0.000  0.000   0.000    0.000      0.000    0.000   \n",
      "phi_offset          -3.076  0.100  -3.258   -2.887      0.005    0.004   \n",
      "p_y_1_given_y_hat_1  0.996  0.003   0.991    0.999      0.000    0.000   \n",
      "p_y_1_given_y_hat_0  0.085  0.007   0.072    0.098      0.000    0.000   \n",
      "...                    ...    ...     ...      ...        ...      ...   \n",
      "p_y[2322]            0.060  0.058   0.001    0.166      0.001    0.001   \n",
      "p_y[2323]            0.100  0.080   0.005    0.248      0.001    0.001   \n",
      "p_y[2324]            0.030  0.024   0.002    0.073      0.000    0.000   \n",
      "p_y[2325]            0.051  0.049   0.002    0.136      0.001    0.001   \n",
      "p_y[2326]            0.107  0.085   0.005    0.268      0.001    0.001   \n",
      "\n",
      "                     ess_bulk  ess_tail  r_hat  \n",
      "p_y_hat_1_given_y_1     431.0    1141.0   1.00  \n",
      "p_y_hat_1_given_y_0   13972.0    3569.0   1.00  \n",
      "phi_offset              387.0     970.0   1.01  \n",
      "p_y_1_given_y_hat_1   13901.0    3500.0   1.00  \n",
      "p_y_1_given_y_hat_0     431.0    1142.0   1.00  \n",
      "...                       ...       ...    ...  \n",
      "p_y[2322]             12439.0    3628.0   1.00  \n",
      "p_y[2323]             12376.0    3569.0   1.00  \n",
      "p_y[2324]             10064.0    3640.0   1.00  \n",
      "p_y[2325]             11455.0    3738.0   1.00  \n",
      "p_y[2326]             12142.0    4452.0   1.00  \n",
      "\n",
      "[2333 rows x 9 columns]\n",
      "Warning: 8 of 2327 empirical p_yhat values are 0; these are being ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3217425/1006087040.py:132: RuntimeWarning: invalid value encountered in divide\n",
      "  empirical_p_yhat = data_to_use['observed_data']['n_classified_positive_by_area'] / data_to_use['observed_data']['n_images_by_area']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['N', 'N_edges', 'node1', 'node2', 'n_images_by_area', 'n_classified_positive_by_area', 'n_classified_positive_annotated_positive_by_area', 'n_classified_positive_annotated_negative_by_area', 'n_classified_negative_annotated_negative_by_area', 'n_classified_negative_annotated_positive_by_area', 'n_non_annotated_by_area', 'n_non_annotated_by_area_classified_positive', 'use_ICAR_prior', 'ICAR_prior_weight'])\n",
      "Building...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Building: found in cache, done.Messages from stanc:\n",
      "Warning: The parameter logit_p_y_1_given_y_hat has no priors. This means\n",
      "    either no prior is provided, or the prior(s) depend on data variables. In\n",
      "    the later case, this may be a false positive.\n",
      "Sampling:   0%\n",
      "Sampling:   0% (1/10000)\n",
      "Sampling:   0% (2/10000)\n",
      "Sampling:   0% (3/10000)\n",
      "Sampling:   0% (4/10000)\n",
      "Sampling:   1% (103/10000)\n",
      "Sampling:   2% (203/10000)\n",
      "Sampling:   3% (303/10000)\n",
      "Sampling:   4% (403/10000)\n",
      "Sampling:   5% (502/10000)\n",
      "Sampling:   6% (602/10000)\n",
      "Sampling:   7% (701/10000)\n",
      "Sampling:   8% (800/10000)\n",
      "Sampling:   9% (900/10000)\n",
      "Sampling:  10% (1000/10000)\n",
      "Sampling:  11% (1100/10000)\n",
      "Sampling:  12% (1200/10000)\n",
      "Sampling:  13% (1300/10000)\n",
      "Sampling:  14% (1400/10000)\n",
      "Sampling:  15% (1500/10000)\n",
      "Sampling:  16% (1600/10000)\n",
      "Sampling:  17% (1700/10000)\n",
      "Sampling:  18% (1800/10000)\n",
      "Sampling:  19% (1900/10000)\n",
      "Sampling:  20% (2000/10000)\n",
      "Sampling:  21% (2100/10000)\n",
      "Sampling:  22% (2200/10000)\n",
      "Sampling:  23% (2300/10000)\n",
      "Sampling:  24% (2400/10000)\n",
      "Sampling:  25% (2501/10000)\n",
      "Sampling:  26% (2601/10000)\n",
      "Sampling:  27% (2701/10000)\n",
      "Sampling:  28% (2801/10000)\n",
      "Sampling:  29% (2901/10000)\n",
      "Sampling:  30% (3001/10000)\n",
      "Sampling:  31% (3101/10000)\n",
      "Sampling:  32% (3201/10000)\n",
      "Sampling:  33% (3301/10000)\n",
      "Sampling:  34% (3401/10000)\n",
      "Sampling:  35% (3502/10000)\n",
      "Sampling:  36% (3601/10000)\n",
      "Sampling:  37% (3701/10000)\n",
      "Sampling:  38% (3801/10000)\n",
      "Sampling:  39% (3901/10000)\n",
      "Sampling:  40% (4002/10000)\n",
      "Sampling:  41% (4103/10000)\n",
      "Sampling:  42% (4202/10000)\n",
      "Sampling:  43% (4302/10000)\n",
      "Sampling:  44% (4401/10000)\n",
      "Sampling:  45% (4500/10000)\n",
      "Sampling:  46% (4600/10000)\n",
      "Sampling:  47% (4700/10000)\n",
      "Sampling:  48% (4800/10000)\n",
      "Sampling:  49% (4900/10000)\n",
      "Sampling:  50% (5000/10000)\n",
      "Sampling:  51% (5100/10000)\n",
      "Sampling:  52% (5200/10000)\n",
      "Sampling:  53% (5300/10000)\n",
      "Sampling:  54% (5400/10000)\n",
      "Sampling:  55% (5500/10000)\n",
      "Sampling:  56% (5600/10000)\n",
      "Sampling:  57% (5700/10000)\n",
      "Sampling:  58% (5800/10000)\n",
      "Sampling:  59% (5900/10000)\n",
      "Sampling:  60% (6000/10000)\n",
      "Sampling:  61% (6100/10000)\n",
      "Sampling:  62% (6200/10000)\n",
      "Sampling:  63% (6300/10000)\n",
      "Sampling:  64% (6400/10000)\n",
      "Sampling:  65% (6500/10000)\n",
      "Sampling:  66% (6600/10000)\n",
      "Sampling:  67% (6700/10000)\n",
      "Sampling:  68% (6800/10000)\n",
      "Sampling:  69% (6900/10000)\n",
      "Sampling:  70% (7000/10000)\n",
      "Sampling:  71% (7100/10000)\n",
      "Sampling:  72% (7200/10000)\n",
      "Sampling:  73% (7300/10000)\n",
      "Sampling:  74% (7400/10000)\n",
      "Sampling:  75% (7500/10000)\n",
      "Sampling:  76% (7600/10000)\n",
      "Sampling:  77% (7700/10000)\n",
      "Sampling:  78% (7800/10000)\n",
      "Sampling:  79% (7900/10000)\n",
      "Sampling:  80% (8000/10000)\n",
      "Sampling:  81% (8100/10000)\n",
      "Sampling:  82% (8200/10000)\n",
      "Sampling:  83% (8300/10000)\n",
      "Sampling:  84% (8400/10000)\n",
      "Sampling:  85% (8500/10000)\n",
      "Sampling:  86% (8600/10000)\n",
      "Sampling:  87% (8700/10000)\n",
      "Sampling:  88% (8800/10000)\n",
      "Sampling:  89% (8900/10000)\n",
      "Sampling:  90% (9000/10000)\n",
      "Sampling:  91% (9100/10000)\n",
      "Sampling:  92% (9200/10000)\n",
      "Sampling:  93% (9300/10000)\n",
      "Sampling:  94% (9400/10000)\n",
      "Sampling:  95% (9500/10000)\n",
      "Sampling:  96% (9600/10000)\n",
      "Sampling:  97% (9700/10000)\n",
      "Sampling:  98% (9800/10000)\n",
      "Sampling:  99% (9900/10000)\n",
      "Sampling: 100% (10000/10000)\n",
      "Sampling: 100% (10000/10000), done.\n",
      "Messages received during sampling:\n",
      "  Gradient evaluation took 0.002873 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 28.73 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.00283 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 28.3 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.002851 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 28.51 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 0.002829 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 28.29 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: binomial_lpmf: Probability parameter[1] is -nan, but must be in the interval [0, 1] (in '/tmp/httpstan_hj6wmh0i/model_2hmgyfhm.stan', line 54, column 2 to column 145)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "/share/ju/conda_virtualenvs/stan/lib/python3.12/site-packages/arviz/stats/diagnostics.py:596: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  (between_chain_variance / within_chain_variance + num_samples - 1) / (num_samples)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  \\\n",
      "p_y_hat_1_given_y_1  0.018  0.002   0.016    0.021      0.000    0.000   \n",
      "p_y_hat_1_given_y_0  0.000  0.000   0.000    0.000      0.000    0.000   \n",
      "phi_offset          -3.076  0.100  -3.258   -2.887      0.005    0.004   \n",
      "p_y_1_given_y_hat_1  0.996  0.003   0.991    0.999      0.000    0.000   \n",
      "p_y_1_given_y_hat_0  0.085  0.007   0.072    0.098      0.000    0.000   \n",
      "...                    ...    ...     ...      ...        ...      ...   \n",
      "p_y[2322]            0.060  0.058   0.001    0.166      0.001    0.001   \n",
      "p_y[2323]            0.100  0.080   0.005    0.248      0.001    0.001   \n",
      "p_y[2324]            0.030  0.024   0.002    0.073      0.000    0.000   \n",
      "p_y[2325]            0.051  0.049   0.002    0.136      0.001    0.001   \n",
      "p_y[2326]            0.107  0.085   0.005    0.268      0.001    0.001   \n",
      "\n",
      "                     ess_bulk  ess_tail  r_hat  \n",
      "p_y_hat_1_given_y_1     431.0    1141.0   1.00  \n",
      "p_y_hat_1_given_y_0   13972.0    3569.0   1.00  \n",
      "phi_offset              387.0     970.0   1.01  \n",
      "p_y_1_given_y_hat_1   13901.0    3500.0   1.00  \n",
      "p_y_1_given_y_hat_0     431.0    1142.0   1.00  \n",
      "...                       ...       ...    ...  \n",
      "p_y[2322]             12439.0    3628.0   1.00  \n",
      "p_y[2323]             12376.0    3569.0   1.00  \n",
      "p_y[2324]             10064.0    3640.0   1.00  \n",
      "p_y[2325]             11455.0    3738.0   1.00  \n",
      "p_y[2326]             12142.0    4452.0   1.00  \n",
      "\n",
      "[2333 rows x 9 columns]\n",
      "Warning: 8 of 2327 empirical p_yhat values are 0; these are being ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3217425/1006087040.py:132: RuntimeWarning: invalid value encountered in divide\n",
      "  empirical_p_yhat = data_to_use['observed_data']['n_classified_positive_by_area'] / data_to_use['observed_data']['n_images_by_area']\n"
     ]
    }
   ],
   "source": [
    "icar_prior_setting = 'none'\n",
    "annotations_have_locations = True\n",
    "use_simulated_data = False\n",
    "assert icar_prior_setting in ['none', 'cheating', 'proper', 'just_model_p_y']\n",
    "stan_code_with_weighted_ICAR_prior = open('stan_models/weighted_ICAR_prior.stan').read()\n",
    "stan_code_proper_car_prior = open('stan_models/proper_car_prior.stan').read()\n",
    "stan_model_uniform_p_y = open('stan_models/uniform_p_y_prior_just_for_debugging.stan').read()\n",
    "stan_code_with_weighted_ICAR_prior_annotations_have_locations = open('stan_models/weighted_ICAR_prior_annotations_have_locations.stan').read()\n",
    "\n",
    "OBSERVED_DATA_FLAG = 'simulated' if use_simulated_data else 'real'\n",
    "\n",
    "for i in range(3):\n",
    "    NUM_WARMUP = 1000#3000\n",
    "    NUM_SAMPLES = 1500\n",
    "    if use_simulated_data:\n",
    "        print(\"Using simulated data\")\n",
    "        N = 1000\n",
    "        data_to_use = util.generate_simulated_data(N=N, \n",
    "                                                images_per_location=1000, \n",
    "                                                total_annotated_classified_negative=500, \n",
    "                                                total_annotated_classified_positive=500, \n",
    "                                                icar_prior_setting=icar_prior_setting, \n",
    "                                                annotations_have_locations=annotations_have_locations)\n",
    "    else:\n",
    "        data_to_use = util.read_real_data(single_compartment_for_debugging=False, annotations_have_locations=annotations_have_locations)\n",
    "    if icar_prior_setting == 'proper':\n",
    "        raise Exception(\"Haven't verified that this model actually works! Need to review it / check on simulated data. No need to review the parts which are identical to the other model.\")\n",
    "        W = np.zeros((N, N))\n",
    "        \n",
    "        for i in range(len(simulated_data['observed_data']['node1'])):\n",
    "            W[simulated_data['observed_data']['node1'][i] - 1, \n",
    "                                simulated_data['observed_data']['node2'][i] - 1] = 1\n",
    "            W[simulated_data['observed_data']['node2'][i] - 1, \n",
    "                                simulated_data['observed_data']['node1'][i] - 1] = 1\n",
    "        del simulated_data['observed_data']['node1']\n",
    "        del simulated_data['observed_data']['node2']\n",
    "        simulated_data['observed_data']['W'] = W\n",
    "        simulated_data['observed_data']['W_n'] = int(W.sum() / 2)\n",
    "        model = stan.build(stan_code_proper_car_prior, data=simulated_data['observed_data'])\n",
    "    elif icar_prior_setting == 'cheating':\n",
    "        data_to_use['observed_data']['use_ICAR_prior'] = 1\n",
    "        data_to_use['observed_data']['ICAR_prior_weight'] = 0.5\n",
    "        if annotations_have_locations:\n",
    "            model = stan.build(stan_code_with_weighted_ICAR_prior_annotations_have_locations, data=data_to_use['observed_data'])\n",
    "        else:\n",
    "            model = stan.build(stan_code_with_weighted_ICAR_prior, data=data_to_use['observed_data'])\n",
    "    elif icar_prior_setting == 'none':\n",
    "        data_to_use['observed_data']['use_ICAR_prior'] = 0\n",
    "        data_to_use['observed_data']['ICAR_prior_weight'] = 0\n",
    "        print(data_to_use['observed_data'].keys())\n",
    "        if annotations_have_locations:\n",
    "            # write jsonified observed data to file for debugging\n",
    "            # need to convert numpy arrays to lists\n",
    "            observed_data_copy = data_to_use['observed_data'].copy()\n",
    "            # observed_data_copy is a dict \n",
    "            for k in observed_data_copy.keys():\n",
    "                if isinstance(observed_data_copy[k], np.ndarray):\n",
    "                    observed_data_copy[k] = observed_data_copy[k].tolist()\n",
    "                # serialized int64 \n",
    "                if isinstance(observed_data_copy[k], np.int64):\n",
    "                    observed_data_copy[k] = int(observed_data_copy[k])\n",
    "                # serialize nd arrays with int 64 elements \n",
    "                if isinstance(observed_data_copy[k], list):\n",
    "                    for i in range(len(observed_data_copy[k])):\n",
    "                        if isinstance(observed_data_copy[k][i], np.int64):\n",
    "                            observed_data_copy[k][i] = int(observed_data_copy[k][i])\n",
    "\n",
    "            with open(f'observed_data_{OBSERVED_DATA_FLAG}_raw.json', 'w') as f:\n",
    "                json.dump(observed_data_copy, f)\n",
    "\n",
    "            # also, dump the sums of each array \n",
    "            with open(f'observed_data_{OBSERVED_DATA_FLAG}_sums.json', 'w') as f:\n",
    "                sums = {}\n",
    "                for k in observed_data_copy.keys():\n",
    "                    sums[k] = float(np.sum(observed_data_copy[k]))\n",
    "                json.dump(sums, f)\n",
    "\n",
    "           \n",
    "            \n",
    "            model = stan.build(stan_code_with_weighted_ICAR_prior_annotations_have_locations, data=data_to_use['observed_data'])\n",
    "        else:\n",
    "            model = stan.build(stan_code_with_weighted_ICAR_prior, data=data_to_use['observed_data'])\n",
    "    elif icar_prior_setting == 'just_model_p_y':\n",
    "        del data_to_use['observed_data']['node1']\n",
    "        del data_to_use['observed_data']['node2']\n",
    "        del data_to_use['observed_data']['N_edges']\n",
    "        model = stan.build(stan_model_uniform_p_y, data=data_to_use['observed_data'])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid icar_prior_options\", icar_prior_setting)\n",
    "    fit = model.sample(num_chains=4, num_warmup=NUM_WARMUP, num_samples=NUM_SAMPLES)\n",
    "    df = fit.to_frame()\n",
    "    if icar_prior_setting == 'just_model_p_y':\n",
    "        print(az.summary(fit, var_names=['p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "                                        'p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "                                        'empirical_p_yhat', 'p_y']))\n",
    "    else:\n",
    "        print(az.summary(fit, var_names=['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', 'phi_offset', \n",
    "                                    'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "                                    'empirical_p_yhat', 'p_y']))\n",
    "        \n",
    "    if use_simulated_data:\n",
    "        inferred_p_y = [df[f'p_y.{i}'].mean() for i in range(1, N + 1)]\n",
    "        plt.scatter(data_to_use['parameters']['p_Y'], inferred_p_y)\n",
    "        plt.title(\"True vs. inferred p_Y, r = %.2f\" %\n",
    "                pearsonr(data_to_use['parameters']['p_Y'], inferred_p_y)[0])\n",
    "        max_val = max(max(data_to_use['parameters']['p_Y']), max(inferred_p_y))\n",
    "        plt.xlabel(\"True p_Y\")\n",
    "        plt.ylabel(\"Inferred p_Y\")\n",
    "        plt.plot([0, max_val], [0, max_val], 'r--')\n",
    "        plt.xlim([0, max_val])\n",
    "        plt.ylim([0, max_val])\n",
    "        plt.figure(figsize=[12, 3])\n",
    "\n",
    "\n",
    "        if icar_prior_setting == 'proper':\n",
    "            param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset', 'alpha', 'tau']\n",
    "        elif icar_prior_setting == 'just_model_p_y':\n",
    "            param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0', \n",
    "            'phi_offset']\n",
    "        else:\n",
    "            param_names = ['p_y_hat_1_given_y_1', 'p_y_hat_1_given_y_0', \n",
    "            'p_y_1_given_y_hat_1', 'p_y_1_given_y_hat_0']\n",
    "        for k in param_names:\n",
    "            plt.subplot(1, len(param_names), param_names.index(k) + 1)\n",
    "            # histogram of posterior samples\n",
    "            plt.hist(df[k], bins=50, density=True)\n",
    "            plt.title(k)\n",
    "            plt.axvline(data_to_use['parameters'][k], color='red')\n",
    "        plt.show()\n",
    "    else:\n",
    "        empirical_p_yhat = data_to_use['observed_data']['n_classified_positive_by_area'] / data_to_use['observed_data']['n_images_by_area']\n",
    "        print(\"Warning: %i of %i empirical p_yhat values are 0; these are being ignored\" % (sum(np.isnan(empirical_p_yhat)), len(empirical_p_yhat)))\n",
    "        inferred_p_y = np.array([df[f'p_y.{i}'].mean() for i in range(1, len(empirical_p_yhat) + 1)])\n",
    "        inferred_p_y_CIs = [df[f'p_y.{i}'].quantile([0.025, 0.975]) for i in range(1, len(empirical_p_yhat) + 1)]\n",
    "        n_images_by_area = data_to_use['observed_data']['n_images_by_area']\n",
    "        # make errorbar plot\n",
    "        image_cutoff = 200\n",
    "\n",
    "        plt.errorbar(empirical_p_yhat[n_images_by_area >= image_cutoff], inferred_p_y[n_images_by_area >= image_cutoff], \n",
    "                    yerr=np.array(inferred_p_y_CIs)[n_images_by_area >= image_cutoff].T, fmt='o', \n",
    "                    color='blue', \n",
    "                    ecolor='lightgray', elinewidth=1, capsize=3, alpha=0.5, \n",
    "                    label=\"n_images_by_area >= %i\" % image_cutoff)\n",
    "\n",
    "        plt.errorbar(empirical_p_yhat[n_images_by_area < image_cutoff], inferred_p_y[n_images_by_area < image_cutoff], \n",
    "                    yerr=np.array(inferred_p_y_CIs)[n_images_by_area < image_cutoff].T, fmt='o', \n",
    "                    color='red', \n",
    "                    ecolor='lightgray', elinewidth=1, capsize=3, alpha=0.5, \n",
    "                    label=\"n_images_by_area < %i\" % image_cutoff)\n",
    "        plt.legend()\n",
    "\n",
    "        # plot prior on p_y as vertical line. \n",
    "        phi_offset = df['phi_offset'].mean()\n",
    "        plt.axhline(expit(phi_offset), color='black', linestyle='--')\n",
    "        is_nan = np.isnan(empirical_p_yhat)\n",
    "        plt.title(\"Correlation between empirical $p(\\\\hat y = 1)$ and inferred $p(y = 1)$, r = %.2f\" % pearsonr(empirical_p_yhat[~is_nan], inferred_p_y[~is_nan])[0])\n",
    "        plt.xlabel(\"empirical $p(\\\\hat y = 1)$\")\n",
    "        plt.ylabel(\"inferred $p(y = 1)$\")\n",
    "        # logarithmic axes\n",
    "        plt.xscale('log')\n",
    "        plt.yscale('log')\n",
    "\n",
    "        plt.savefig(f\"correlation_{icar_prior_setting}_{annotations_have_locations}_{i}.png\")\n",
    "        plt.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_stan_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
