{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:49:30 - analysis-df-assembly - INFO - Modules loaded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import geopandas as gpd \n",
    "import numpy as np \n",
    "import json \n",
    "from glob import glob \n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from logger import setup_logger\n",
    "logger = setup_logger(\"analysis-df-assembly\")\n",
    "logger.setLevel(\"INFO\")\n",
    "\n",
    "WGS='EPSG:4326'\n",
    "PROJ='EPSG:2263'\n",
    "\n",
    "import os \n",
    "\n",
    "logger.info(\"Modules loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERNAL_DATA = True \n",
    "\n",
    "USE_SMOOTHING = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICAR_NONE_RUN='../runs/icar_none/simulated_False/ahl_True/20241021-1038'\n",
    "ICAR_CHEATING_RUN='../runs/icar_cheating/simulated_False/ahl_True/20241022-1130'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:49:30 - analysis-df-assembly - INFO - Found 2 ICAR_NONE estimates and 3 ICAR_CHEATING estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ICAR_NONE_ESTIMATES = glob(f\"{ICAR_NONE_RUN}/estimate*.csv\")\n",
    "ICAR_CHEATING_ESTIMATES = glob(f\"{ICAR_CHEATING_RUN}/estimate*.csv\")\n",
    "logger.info(f\"Found {len(ICAR_NONE_ESTIMATES)} ICAR_NONE estimates and {len(ICAR_CHEATING_ESTIMATES)} ICAR_CHEATING estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_cheating_estimates = {} \n",
    "for f in ICAR_CHEATING_ESTIMATES:\n",
    "    df = pd.read_csv(f)\n",
    "    df['tract_id'] = df['tract_id'].astype(int).astype(str)\n",
    "    icar_cheating_estimates[os.path.splitext(os.path.basename(f))[0]] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "icar_none_estimates = {} \n",
    "for f in ICAR_NONE_ESTIMATES:\n",
    "    df = pd.read_csv(f)\n",
    "    df['tract_id'] = df['tract_id'].astype(int).astype(str)\n",
    "    icar_none_estimates[os.path.splitext(os.path.basename(f)[0])] = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:49:31 - analysis-df-assembly - INFO - Using smoothed estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if USE_SMOOTHING: \n",
    "    icar_model_estimates = icar_cheating_estimates\n",
    "    logger.info(\"Using smoothed estimates.\")\n",
    "else:\n",
    "    icar_model_estimates = icar_none_estimates\n",
    "    logger.info(\"Using unsmoothed estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:49:31 - analysis-df-assembly - INFO - Loaded NYC CT shapefile with 2325 CTs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc = gpd.read_file('geo/data/ct-nyc-wi-2020.geojson')\n",
    "\n",
    "\n",
    "TO_DROP = ['OBJECTID','BoroCode','CT2020','CDEligibil','NTA2020','CDTA2020','Shape__Area','Shape__Length','geometry']\n",
    "ct_nyc.drop(columns=TO_DROP, inplace=True)\n",
    "\n",
    "logger.info(f\"Loaded NYC CT shapefile with {len(ct_nyc.index)} CTs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CTLabel', 'BoroName', 'BoroCT2020', 'NTAName', 'CDTANAME', 'GEOID',\n",
       "       'PUMA'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_nyc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:49:31 - analysis-df-assembly - INFO - Loaded NYC CT (water clipped) shapefile with 2327 CTs.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc_clip = gpd.read_file('geo/data/ct-nyc-2020.geojson')\n",
    "logger.info(f\"Loaded NYC CT (water clipped) shapefile with {len(ct_nyc_clip.index)} CTs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:49:31 - analysis-df-assembly - INFO - Merged NYC CT shapefile with icar model estimates.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_p_y'], left_on='GEOID', right_on='tract_id', suffixes=('_ct', '_p_y'))\n",
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_at_least_one_positive_image_by_area'], left_on='GEOID', right_on='tract_id', suffixes=('_ct', '_p_alop'))\n",
    "ct_nyc = ct_nyc.merge(icar_model_estimates['estimate_at_least_one_positive_image_by_area_if_you_have_100_images'], left_on='GEOID', right_on='tract_id', suffixes=('_ct', '_p_alop_100'))\n",
    "\n",
    "# drop empirical_estimate_* cols \n",
    "TO_DROP = [c for c in ct_nyc.columns if 'empirical_estimate_' in c]\n",
    "ct_nyc.drop(columns=TO_DROP, inplace=True)\n",
    "\n",
    "logger.info(f\"Merged NYC CT shapefile with icar model estimates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dp05_nyc_md = pd.read_json('demo/data/acs22_dp05_md.json')\n",
    "\n",
    "# Normalize the 'variables' column in the JSON\n",
    "dp05_nyc_md = pd.json_normalize(dp05_nyc_md['variables']).set_index(dp05_nyc_md.index)\n",
    "\n",
    "# Parse out the 'label' column\n",
    "# In all rows of the 'label', get the lowest and highest number of '!!'\n",
    "min_sep = min(dp05_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "max_sep = max(dp05_nyc_md['label'].apply(lambda x: x.count('!!')))\n",
    "\n",
    "# Create 'desc_i' columns for each level of '!!'\n",
    "for i in range(min_sep + 1, max_sep + 2):  # Adjusting range to account for correct indexing\n",
    "    dp05_nyc_md[f'desc_{i}'] = dp05_nyc_md['label'].apply(\n",
    "        lambda x: x.split('!!')[i-1] if len(x.split('!!')) >= i else None\n",
    "    )\n",
    "\n",
    "# drop TO_DROP \n",
    "TO_DROP = ['label','concept','predicateType','group','limit','predicateOnly']\n",
    "dp05_nyc_md = dp05_nyc_md.drop(columns=TO_DROP)\n",
    "\n",
    "desc_1_filter = ['Estimate']\n",
    "dp05_nyc_md = dp05_nyc_md[dp05_nyc_md['desc_1'].isin(desc_1_filter)]\n",
    "\n",
    "# Output the modified dataframe\n",
    "# display all rows \n",
    "dp05_nyc_md = dp05_nyc_md.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nhl_white_alone</th>\n",
       "      <th>nhl_black_alone</th>\n",
       "      <th>hispanic_alone</th>\n",
       "      <th>nhl_asian_alone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tract_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36005000100</th>\n",
       "      <td>1098</td>\n",
       "      <td>2000</td>\n",
       "      <td>1172</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000200</th>\n",
       "      <td>83</td>\n",
       "      <td>1281</td>\n",
       "      <td>3109</td>\n",
       "      <td>299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005000400</th>\n",
       "      <td>283</td>\n",
       "      <td>1559</td>\n",
       "      <td>4212</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001600</th>\n",
       "      <td>106</td>\n",
       "      <td>2132</td>\n",
       "      <td>3507</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36005001901</th>\n",
       "      <td>306</td>\n",
       "      <td>942</td>\n",
       "      <td>842</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085030302</th>\n",
       "      <td>2209</td>\n",
       "      <td>1568</td>\n",
       "      <td>1625</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031901</th>\n",
       "      <td>289</td>\n",
       "      <td>1626</td>\n",
       "      <td>1469</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085031902</th>\n",
       "      <td>473</td>\n",
       "      <td>2388</td>\n",
       "      <td>1913</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085032300</th>\n",
       "      <td>109</td>\n",
       "      <td>421</td>\n",
       "      <td>394</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36085990100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2327 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0           nhl_white_alone nhl_black_alone hispanic_alone nhl_asian_alone\n",
       "tract_id                                                                  \n",
       "36005000100            1098            2000           1172             123\n",
       "36005000200              83            1281           3109             299\n",
       "36005000400             283            1559           4212             103\n",
       "36005001600             106            2132           3507             148\n",
       "36005001901             306             942            842               0\n",
       "...                     ...             ...            ...             ...\n",
       "36085030302            2209            1568           1625             918\n",
       "36085031901             289            1626           1469             224\n",
       "36085031902             473            2388           1913             217\n",
       "36085032300             109             421            394              21\n",
       "36085990100               0               0              0               0\n",
       "\n",
       "[2327 rows x 4 columns]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp05_nyc = pd.read_json('demo/data/acs22_dp05.json', orient='records')\n",
    "\n",
    "dp05_nyc.columns = dp05_nyc.iloc[0]\n",
    "dp05_nyc = dp05_nyc[1:]\n",
    "\n",
    "dp05_nyc['tract_id'] = dp05_nyc['GEO_ID'].str.split('US', expand=True)[1]\n",
    "\n",
    "RACE_COLS = {\n",
    "    'DP05_0079E': 'nhl_white_alone', \n",
    "    'DP05_0080E': 'nhl_black_alone', \n",
    "    'DP05_0073E': 'hispanic_alone', \n",
    "    'DP05_0082E': 'nhl_asian_alone'\n",
    "}\n",
    "\n",
    "race_nyc = dp05_nyc[list(RACE_COLS.keys())]\n",
    "race_nyc.columns = race_nyc.columns.map(lambda x: RACE_COLS[x])\n",
    "race_nyc.index = dp05_nyc['tract_id']\n",
    "race_nyc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.merge(race_nyc, left_on='GEOID', right_index=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_nyc = ct_nyc.set_index('GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_ct_nyc = gpd.read_file('geo/data/ct-nyc-wi-2020.geojson').to_crs(PROJ)[['GEOID','geometry']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:49:33 - analysis-df-assembly - INFO - Loaded and processed Floodnet sensor data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# FLOODNET \n",
    "floodnet_sensor = pd.read_csv('flooding/static/floodnet-flood-sensor-sep-2023.csv', engine='pyarrow')\n",
    "floodnet_tide = pd.read_csv('flooding/static/floodnet-tide-sep-2023.csv', engine='pyarrow')\n",
    "floodnet_weather = pd.read_csv('flooding/static/floodnet-weather-sep-2023.csv', engine='pyarrow')\n",
    "\n",
    "\n",
    "all_floodnet_sensors_geo = pd.concat([floodnet_sensor.groupby('deployment_id').first()[['lat','lon']].reset_index(), floodnet_tide.groupby('sensor_id').first()[['lat','lon']].reset_index(), floodnet_weather.groupby('sensor_id').first()[['lat','lon']].reset_index()], axis=0)\n",
    "\n",
    "all_floodnet_sensor_geo = gpd.GeoDataFrame(all_floodnet_sensors_geo, geometry=gpd.points_from_xy(all_floodnet_sensors_geo.lon, all_floodnet_sensors_geo.lat), crs='EPSG:4326').to_crs(2263)\n",
    "\n",
    "del floodnet_sensor, floodnet_tide, floodnet_weather\n",
    "\n",
    "logger.info(\"Loaded and processed Floodnet sensor data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:49:33 - analysis-df-assembly - INFO - Merged Floodnet sensor data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 11:49:33 - analysis-df-assembly - INFO - count    2325.000000\n",
      "mean        0.036129\n",
      "std         0.272811\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         6.000000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the number of floodnet sensors in each census tract in gdf_ct_nyc, including tracts with 0 sensors\n",
    "ct_nyc['n_sensors'] = gpd.sjoin(gdf_ct_nyc, all_floodnet_sensor_geo).groupby('GEOID').size().reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "logger.info(\"Merged Floodnet sensor data with NYC CT shapefile.\")\n",
    "logger.info(ct_nyc['n_sensors'].describe().to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEP STORMWATER \n",
    "dep_light = gpd.read_file('flooding/data/NYCFloodStormwaterFloodMaps/NYC Stormwater Flood Map - Extreme Flood (3.66 inches per hr) with 2080 Sea Level Rise/NYC_Stormwater_Flood_Map_Extreme_Flood_3_66_inches_per_hr_with_2080_Sea_Level_Rise.gdb').to_crs(PROJ)\n",
    "dep_moderate = gpd.read_file('flooding/data/NYCFloodStormwaterFloodMaps/NYC Stormwater Flood Map - Moderate Flood (2.13 inches per hr) with Current Sea Levels/NYC_Stormwater_Flood_Map_Moderate_Flood_2_13_inches_per_hr_with_Current_Sea_Levels.gdb').to_crs(PROJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = {} \n",
    "for i, row in dep_light.iterrows():\n",
    "    idx = 0\n",
    "    for polygon in row['geometry'].geoms:\n",
    "        polygons[f'{row[\"Flooding_Category\"]}_{idx}'] = polygon\n",
    "        idx += 1\n",
    "\n",
    "# dataframe from dict \n",
    "dep_light_flattened = gpd.GeoDataFrame(polygons, index=['geometry']).T\n",
    "# flooding category is the first part of the index\n",
    "\n",
    "\n",
    "dep_light_flattened.set_geometry('geometry', inplace=True)\n",
    "dep_light_flattened.crs = dep_light.crs\n",
    "\n",
    "dep_light_flattened['Flooding_Category'] = dep_light_flattened.index.str.split('_').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "polygons = {}\n",
    "for i, row in dep_moderate.iterrows():\n",
    "    idx = 0\n",
    "    for polygon in row['geometry'].geoms:\n",
    "        polygons[f'{row[\"Flooding_Category\"]}_{idx}'] = polygon\n",
    "        idx += 1\n",
    "    \n",
    "# dataframe from dict\n",
    "dep_moderate_flattened = gpd.GeoDataFrame(polygons, index=['geometry']).T\n",
    "\n",
    "dep_moderate_flattened.set_geometry('geometry', inplace=True)\n",
    "dep_moderate_flattened.crs = dep_moderate.crs\n",
    "\n",
    "dep_moderate_flattened['Flooding_Category'] = dep_moderate_flattened.index.str.split('_').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:50:31 - analysis-df-assembly - INFO - Merged DEP stormwater data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 11:50:31 - analysis-df-assembly - INFO - \n",
      "       dep_light_1_area  dep_light_2_area  dep_light_3_area  dep_moderate_1_area  dep_moderate_2_area\n",
      "count      2.325000e+03      2.325000e+03      2.325000e+03          2325.000000          2325.000000\n",
      "mean       1.704254e+05      1.305767e+05      1.375244e+05         33568.736899         17255.142646\n",
      "std        1.926640e+05      2.015365e+05      7.969362e+05         77947.720054         49174.975593\n",
      "min        0.000000e+00      0.000000e+00      0.000000e+00             0.000000             0.000000\n",
      "25%        4.756178e+04      1.274651e+04      0.000000e+00             0.000000             0.000000\n",
      "50%        1.176228e+05      5.511028e+04      0.000000e+00          5211.095209             0.000000\n",
      "75%        2.227261e+05      1.648135e+05      0.000000e+00         33654.810667          9705.671683\n",
      "max        1.653186e+06      2.586617e+06      1.279555e+07        886875.919586        660828.326796\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# get the total area of light and moderate flooding in each ct in gdf_ct_nyc \n",
    "dep_light_flattened_1 = dep_light_flattened[dep_light_flattened['Flooding_Category'] == 1]\n",
    "dep_light_flattened_2 = dep_light_flattened[dep_light_flattened['Flooding_Category'] == 2]\n",
    "dep_light_flattened_3 = dep_light_flattened[dep_light_flattened['Flooding_Category'] == 3]\n",
    "ct_nyc['dep_light_1_area'] = gpd.overlay(gdf_ct_nyc, dep_light_flattened_1, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_light_2_area'] = gpd.overlay(gdf_ct_nyc, dep_light_flattened_2, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_light_3_area'] = gpd.overlay(gdf_ct_nyc, dep_light_flattened_3, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "dep_moderate_flattened_1 = dep_moderate_flattened[dep_moderate_flattened['Flooding_Category'] == 1]\n",
    "dep_moderate_flattened_2 = dep_moderate_flattened[dep_moderate_flattened['Flooding_Category'] == 2]\n",
    "ct_nyc['dep_moderate_1_area'] = gpd.overlay(gdf_ct_nyc, dep_moderate_flattened_1, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "ct_nyc['dep_moderate_2_area'] = gpd.overlay(gdf_ct_nyc, dep_moderate_flattened_2, how='intersection').groupby('GEOID')['geometry'].apply(lambda geom: geom.area.sum()).reindex(ct_nyc.index).fillna(0)\n",
    "\n",
    "logger.info(\"Merged DEP stormwater data with NYC CT shapefile.\")\n",
    "\n",
    "logger.info(\"\\n\"+ct_nyc[['dep_light_1_area','dep_light_2_area','dep_light_3_area','dep_moderate_1_area','dep_moderate_2_area']].describe().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:50:31 - analysis-df-assembly - INFO - Loaded and processed 311 data from September 29, 2023.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 311 from September 29 Flood \n",
    "nyc311_sep29 = pd.read_csv('flooding/data/nyc311_flooding_sep29.csv').dropna(subset=['latitude','longitude'])\n",
    "\n",
    "nyc311_sep29 = gpd.GeoDataFrame(nyc311_sep29, geometry=gpd.points_from_xy(nyc311_sep29.longitude, nyc311_sep29.latitude), crs=WGS).to_crs(PROJ)\n",
    "\n",
    "logger.info(\"Loaded and processed 311 data from September 29, 2023.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "descriptor\n",
       "Sewer Backup (Use Comments) (SA)                    1081\n",
       "Street Flooding (SJ)                                 625\n",
       "Catch Basin Clogged/Flooding (Use Comments) (SC)     429\n",
       "Manhole Overflow (Use Comments) (SA1)                 35\n",
       "Highway Flooding (SH)                                  1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyc311_sep29['descriptor'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:50:35 - analysis-df-assembly - INFO - Merged 311 data with NYC CT shapefile.\u001b[0m\n",
      "\u001b[34m2024-10-23 11:50:35 - analysis-df-assembly - INFO - sewer_backup_311c                    1081\n",
      "street_flooding_311c                  625\n",
      "catch_basin_clogged/flooding_311c     429\n",
      "manhole_overflow_311c                  35\n",
      "highway_flooding_311c                   1\n",
      "dtype: int64\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# for each unique val of descriptor, create a column in gdf_ct_nyc with the count of 311 calls of that descriptor type in each tract \n",
    "for descriptor in nyc311_sep29['descriptor'].unique():\n",
    "    # human-readable column name \n",
    "    # remove anything inside () \n",
    "    desc = descriptor.split('(')[0].strip()\n",
    "    desc = desc.lower().replace(' ', '_') + '_311c'\n",
    "\n",
    "    gdf_ct_nyc[desc] = gdf_ct_nyc['geometry'].apply(lambda x: nyc311_sep29[nyc311_sep29['descriptor'] == descriptor].within(x).sum())\n",
    "\n",
    "logger.info(\"Merged 311 data with NYC CT shapefile.\")\n",
    "logger.info(gdf_ct_nyc[[c for c in gdf_ct_nyc.columns if '_311c' in c]].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge ct_nyc with gdf_ct_nyc\n",
    "ct_nyc = ct_nyc.merge(gdf_ct_nyc, on='GEOID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-10-23 11:50:46 - analysis-df-assembly - SUCCESS - No N/A values found in columns.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "COLS_ALLOWED_NA_VALS = ['empirical_estimate']\n",
    "def na_validation(df, cols_allowed_na_vals):\n",
    "    for c in df.columns:\n",
    "        if c in cols_allowed_na_vals:\n",
    "            continue\n",
    "        if df[c].isna().sum() > 0:\n",
    "            logger.error(f\"Column {c} has {df[c].isna().sum()} NA values.\")\n",
    "    else: \n",
    "        logger.success(\"No N/A values found in columns.\")\n",
    "na_validation(ct_nyc, COLS_ALLOWED_NA_VALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m2024-10-23 11:59:34 - analysis-df-assembly - INFO - Dropped columns: {'n_images_by_area_p_alop', 'n_images_by_area_ct'}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# final cleaning \n",
    "TO_DROP = ['tract_id', 'n_images_by_area_']\n",
    "# drop all columns that match regex of entry in list \n",
    "current_cols = ct_nyc.columns\n",
    "for c in TO_DROP:\n",
    "    ct_nyc = ct_nyc.loc[:, ~ct_nyc.columns.str.contains(c)]\n",
    "\n",
    "logger.info(f\"Dropped columns: {set(current_cols) - set(ct_nyc.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_y</th>\n",
       "      <th>p_y_CI_lower</th>\n",
       "      <th>p_y_CI_upper</th>\n",
       "      <th>n_images_by_area_ct</th>\n",
       "      <th>at_least_one_positive_image_by_area</th>\n",
       "      <th>at_least_one_positive_image_by_area_CI_lower</th>\n",
       "      <th>at_least_one_positive_image_by_area_CI_upper</th>\n",
       "      <th>n_images_by_area_p_alop</th>\n",
       "      <th>empirical_estimate</th>\n",
       "      <th>at_least_one_positive_image_by_area_if_you_have_100_images</th>\n",
       "      <th>...</th>\n",
       "      <th>dep_light_1_area</th>\n",
       "      <th>dep_light_2_area</th>\n",
       "      <th>dep_light_3_area</th>\n",
       "      <th>dep_moderate_1_area</th>\n",
       "      <th>dep_moderate_2_area</th>\n",
       "      <th>sewer_backup_311c</th>\n",
       "      <th>street_flooding_311c</th>\n",
       "      <th>catch_basin_clogged/flooding_311c</th>\n",
       "      <th>manhole_overflow_311c</th>\n",
       "      <th>highway_flooding_311c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2319.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.325000e+03</td>\n",
       "      <td>2.325000e+03</td>\n",
       "      <td>2.325000e+03</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "      <td>2325.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.003020</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>398.370753</td>\n",
       "      <td>0.492648</td>\n",
       "      <td>0.299645</td>\n",
       "      <td>0.713894</td>\n",
       "      <td>398.370753</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.290829</td>\n",
       "      <td>...</td>\n",
       "      <td>1.704254e+05</td>\n",
       "      <td>1.305767e+05</td>\n",
       "      <td>1.375244e+05</td>\n",
       "      <td>33568.736899</td>\n",
       "      <td>17255.142646</td>\n",
       "      <td>0.464946</td>\n",
       "      <td>0.268817</td>\n",
       "      <td>0.184516</td>\n",
       "      <td>0.015054</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.004561</td>\n",
       "      <td>742.306729</td>\n",
       "      <td>0.266477</td>\n",
       "      <td>0.266153</td>\n",
       "      <td>0.243191</td>\n",
       "      <td>742.306729</td>\n",
       "      <td>0.007372</td>\n",
       "      <td>0.203451</td>\n",
       "      <td>...</td>\n",
       "      <td>1.926640e+05</td>\n",
       "      <td>2.015365e+05</td>\n",
       "      <td>7.969362e+05</td>\n",
       "      <td>77947.720054</td>\n",
       "      <td>49174.975593</td>\n",
       "      <td>1.346314</td>\n",
       "      <td>1.028633</td>\n",
       "      <td>0.527163</td>\n",
       "      <td>0.128665</td>\n",
       "      <td>0.020739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.285737</td>\n",
       "      <td>0.118919</td>\n",
       "      <td>0.540621</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.215720</td>\n",
       "      <td>...</td>\n",
       "      <td>4.756178e+04</td>\n",
       "      <td>1.274651e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.005960</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>0.438154</td>\n",
       "      <td>0.201886</td>\n",
       "      <td>0.743220</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229950</td>\n",
       "      <td>...</td>\n",
       "      <td>1.176228e+05</td>\n",
       "      <td>5.511028e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5211.095209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.002922</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>0.684235</td>\n",
       "      <td>0.386835</td>\n",
       "      <td>0.942545</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.248656</td>\n",
       "      <td>...</td>\n",
       "      <td>2.227261e+05</td>\n",
       "      <td>1.648135e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>33654.810667</td>\n",
       "      <td>9705.671683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.046621</td>\n",
       "      <td>0.019754</td>\n",
       "      <td>0.092098</td>\n",
       "      <td>20714.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20714.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653186e+06</td>\n",
       "      <td>2.586617e+06</td>\n",
       "      <td>1.279555e+07</td>\n",
       "      <td>886875.919586</td>\n",
       "      <td>660828.326796</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               p_y  p_y_CI_lower  p_y_CI_upper  n_images_by_area_ct  \\\n",
       "count  2325.000000   2325.000000   2325.000000          2325.000000   \n",
       "mean      0.003020      0.001110      0.006660           398.370753   \n",
       "std       0.002229      0.000909      0.004561           742.306729   \n",
       "min       0.001688      0.000315      0.003550             0.000000   \n",
       "25%       0.002494      0.000900      0.005455           124.000000   \n",
       "50%       0.002687      0.000978      0.005960           217.000000   \n",
       "75%       0.002922      0.001078      0.006572           440.000000   \n",
       "max       0.046621      0.019754      0.092098         20714.000000   \n",
       "\n",
       "       at_least_one_positive_image_by_area  \\\n",
       "count                          2325.000000   \n",
       "mean                              0.492648   \n",
       "std                               0.266477   \n",
       "min                               0.000000   \n",
       "25%                               0.285737   \n",
       "50%                               0.438154   \n",
       "75%                               0.684235   \n",
       "max                               1.000000   \n",
       "\n",
       "       at_least_one_positive_image_by_area_CI_lower  \\\n",
       "count                                   2325.000000   \n",
       "mean                                       0.299645   \n",
       "std                                        0.266153   \n",
       "min                                        0.000000   \n",
       "25%                                        0.118919   \n",
       "50%                                        0.201886   \n",
       "75%                                        0.386835   \n",
       "max                                        1.000000   \n",
       "\n",
       "       at_least_one_positive_image_by_area_CI_upper  n_images_by_area_p_alop  \\\n",
       "count                                   2325.000000              2325.000000   \n",
       "mean                                       0.713894               398.370753   \n",
       "std                                        0.243191               742.306729   \n",
       "min                                        0.000000                 0.000000   \n",
       "25%                                        0.540621               124.000000   \n",
       "50%                                        0.743220               217.000000   \n",
       "75%                                        0.942545               440.000000   \n",
       "max                                        1.000000             20714.000000   \n",
       "\n",
       "       empirical_estimate  \\\n",
       "count         2319.000000   \n",
       "mean             0.001591   \n",
       "std              0.007372   \n",
       "min              0.000000   \n",
       "25%              0.000000   \n",
       "50%              0.000000   \n",
       "75%              0.000000   \n",
       "max              0.166667   \n",
       "\n",
       "       at_least_one_positive_image_by_area_if_you_have_100_images  ...  \\\n",
       "count                                        2325.000000           ...   \n",
       "mean                                            0.290829           ...   \n",
       "std                                             0.203451           ...   \n",
       "min                                             0.153201           ...   \n",
       "25%                                             0.215720           ...   \n",
       "50%                                             0.229950           ...   \n",
       "75%                                             0.248656           ...   \n",
       "max                                             1.000000           ...   \n",
       "\n",
       "       dep_light_1_area  dep_light_2_area  dep_light_3_area  \\\n",
       "count      2.325000e+03      2.325000e+03      2.325000e+03   \n",
       "mean       1.704254e+05      1.305767e+05      1.375244e+05   \n",
       "std        1.926640e+05      2.015365e+05      7.969362e+05   \n",
       "min        0.000000e+00      0.000000e+00      0.000000e+00   \n",
       "25%        4.756178e+04      1.274651e+04      0.000000e+00   \n",
       "50%        1.176228e+05      5.511028e+04      0.000000e+00   \n",
       "75%        2.227261e+05      1.648135e+05      0.000000e+00   \n",
       "max        1.653186e+06      2.586617e+06      1.279555e+07   \n",
       "\n",
       "       dep_moderate_1_area  dep_moderate_2_area  sewer_backup_311c  \\\n",
       "count          2325.000000          2325.000000        2325.000000   \n",
       "mean          33568.736899         17255.142646           0.464946   \n",
       "std           77947.720054         49174.975593           1.346314   \n",
       "min               0.000000             0.000000           0.000000   \n",
       "25%               0.000000             0.000000           0.000000   \n",
       "50%            5211.095209             0.000000           0.000000   \n",
       "75%           33654.810667          9705.671683           0.000000   \n",
       "max          886875.919586        660828.326796          22.000000   \n",
       "\n",
       "       street_flooding_311c  catch_basin_clogged/flooding_311c  \\\n",
       "count           2325.000000                        2325.000000   \n",
       "mean               0.268817                           0.184516   \n",
       "std                1.028633                           0.527163   \n",
       "min                0.000000                           0.000000   \n",
       "25%                0.000000                           0.000000   \n",
       "50%                0.000000                           0.000000   \n",
       "75%                0.000000                           0.000000   \n",
       "max               26.000000                           6.000000   \n",
       "\n",
       "       manhole_overflow_311c  highway_flooding_311c  \n",
       "count            2325.000000            2325.000000  \n",
       "mean                0.015054               0.000430  \n",
       "std                 0.128665               0.020739  \n",
       "min                 0.000000               0.000000  \n",
       "25%                 0.000000               0.000000  \n",
       "50%                 0.000000               0.000000  \n",
       "75%                 0.000000               0.000000  \n",
       "max                 2.000000               1.000000  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_nyc.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = pd.to_datetime('today').strftime('%m%d%Y')\n",
    "ct_nyc.to_csv(f'analysis_df_{todays_date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
