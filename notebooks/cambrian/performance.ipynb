{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "from tqdm import tqdm \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('anl_set_jun16_w_extra_from_original_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'image_path', 'split', 'class', 'q0', 'q1', 'q2', 'q3',\n",
       "       'q4', 'q5', 'q6', 'response_0', 'response_1', 'response_2',\n",
       "       'response_3', 'response_4', 'response_5', 'response_6'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gt\n",
       "0    4186\n",
       "1    1060\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['gt'] = df['class'].apply(lambda x: 0 if x == 'drivable' else 1)\n",
    "df['gt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text, analyzer=NaiveBayesAnalyzer())\n",
    "    return blob.sentiment[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_sentiment(text):\n",
    "    # if first word is yes, 1\n",
    "    # if first word is no, 0\n",
    "    # else, 0.5\n",
    "    text = str(text).lower()\n",
    "    # strip lspace\n",
    "    text = text.lstrip()\n",
    "    if text.startswith('yes'):\n",
    "        return 1\n",
    "    elif text.startswith('no'):\n",
    "        return 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5246/5246 [00:00<00:00, 613435.52it/s]\n",
      "100%|██████████| 5246/5246 [00:00<00:00, 632006.86it/s]\n",
      "100%|██████████| 5246/5246 [00:00<00:00, 643240.24it/s]\n",
      "100%|██████████| 5246/5246 [00:00<00:00, 633480.70it/s]\n",
      "100%|██████████| 5246/5246 [00:00<00:00, 656208.25it/s]\n",
      "100%|██████████| 5246/5246 [00:00<00:00, 661197.15it/s]\n",
      "100%|██████████| 5246/5246 [00:00<00:00, 675114.10it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,7):\n",
    "    df[f'sentiment_{i}'] = df[f'response_{i}'].progress_apply(quick_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['response_0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample size per class\n",
    "sample_size_per_class = 500\n",
    "\n",
    "# Sample 500 from each class\n",
    "df_class_0 = df[df['gt'] == 0].sample(n=sample_size_per_class, random_state=1, replace=True)\n",
    "df_class_1 = df[df['gt'] == 1].sample(n=sample_size_per_class, random_state=1, replace=True)\n",
    "\n",
    "# Concatenate the samples\n",
    "df = pd.concat([df_class_0, df_class_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gt\n",
       "0    500\n",
       "1    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['gt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0: Does this image show a flooded street?\n",
      "Precision: 0.5788849347568209\n",
      "Recall: 0.976\n",
      "F1: 0.7267311988086375\n",
      "\n",
      "Question 1: Does this image show more than a foot of standing water?\n",
      "Precision: 1.0\n",
      "Recall: 0.634\n",
      "F1: 0.7760097919216646\n",
      "\n",
      "Question 2: Is the street in this image flooded?\n",
      "Precision: 0.7452229299363057\n",
      "Recall: 0.936\n",
      "F1: 0.8297872340425532\n",
      "\n",
      "Question 3: Could a car drive through the water in this image?\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "\n",
      "Question 4: Does this image show a visible street?\n",
      "Precision: 0.4959677419354839\n",
      "Recall: 0.984\n",
      "F1: 0.6595174262734583\n",
      "\n",
      "Question 5: Is there any visible street in this image?\n",
      "Precision: 0.46316964285714285\n",
      "Recall: 0.83\n",
      "F1: 0.5945558739255014\n",
      "\n",
      "Question 6: Is the view from windshield in this image too obstructed?\n",
      "Precision: 0.47489539748953974\n",
      "Recall: 0.454\n",
      "F1: 0.46421267893660534\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/ju/conda_virtualenvs/cambrian/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# get performance for each question\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "for i in range(0,7):\n",
    "    print(f'Question {i}: {df.loc[0, f\"q{i}\"]}')\n",
    "    print(f'Precision: {precision_score(df[\"gt\"], df[f\"sentiment_{i}\"])}')\n",
    "    print(f'Recall: {recall_score(df[\"gt\"], df[f\"sentiment_{i}\"])}')\n",
    "    print(f'F1: {f1_score(df[\"gt\"], df[f\"sentiment_{i}\"])}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions 4,5,6 act as filters for 0,1,2,3; \n",
    "# if 4,5,6 is positive, then 0,1,2,3 should not even be possible to assess \n",
    "# if 4,5,6 is negative, then 0,1,2,3 should be assessed\n",
    "\n",
    "def filter_4_5_6(row):\n",
    "    if row['sentiment_6'] == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return row['gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0: Does this image show a flooded street?\n",
      "Precision: 0.3190984578884935\n",
      "Recall: 0.9853479853479854\n",
      "F1: 0.48207885304659504\n",
      "\n",
      "Question 1: Does this image show more than a foot of standing water?\n",
      "Precision: 0.5425867507886435\n",
      "Recall: 0.63003663003663\n",
      "F1: 0.5830508474576271\n",
      "\n",
      "Question 2: Is the street in this image flooded?\n",
      "Precision: 0.4124203821656051\n",
      "Recall: 0.9487179487179487\n",
      "F1: 0.5749167591564928\n",
      "\n",
      "Question 3: Could a car drive through the water in this image?\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "\n",
      "Question 4: Does this image show a visible street?\n",
      "Precision: 0.2691532258064516\n",
      "Recall: 0.978021978021978\n",
      "F1: 0.42213438735177866\n",
      "\n",
      "Question 5: Is there any visible street in this image?\n",
      "Precision: 0.24553571428571427\n",
      "Recall: 0.8058608058608059\n",
      "F1: 0.3763900769888794\n",
      "\n",
      "Question 6: Is the view from windshield in this image too obstructed?\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/ju/conda_virtualenvs/cambrian/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,7):\n",
    "    df[f'filtered_gt_{i}'] = df.apply(filter_4_5_6, axis=1)\n",
    "\n",
    "for i in range(0,7):\n",
    "    print(f'Question {i}: {df.loc[0, f\"q{i}\"]}')\n",
    "    print(f'Precision: {precision_score(df[\"filtered_gt_6\"], df[f\"sentiment_{i}\"])}')\n",
    "    print(f'Recall: {recall_score(df[\"filtered_gt_6\"], df[f\"sentiment_{i}\"])}')\n",
    "    print(f'F1: {f1_score(df[\"filtered_gt_6\"], df[f\"sentiment_{i}\"])}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
